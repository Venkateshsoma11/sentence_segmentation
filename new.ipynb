{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Get the latest tutorials on SysAdmin and open source topics.\n",
      "  \n",
      "\n",
      "Many data analysis, big data, and machine learning projects require scraping websites to gather the data that you’ll be working with. The Python programming language is widely used in the data science community, and therefore has an ecosystem of modules and tools that you can use in your own projects. In this tutorial we will be focusing on the Beautiful Soup module. \n",
      "\n",
      "Beautiful Soup, an allusion to the Mock Turtle’s song found in Chapter 10 of Lewis Carroll’s Alice’s Adventures in Wonderland, is a Python library that allows for quick turnaround on web scraping projects. Currently available as Beautiful Soup 4 and compatible with both Python 2.7 and Python 3, Beautiful Soup creates a parse tree from parsed HTML and XML documents (including documents with non-closed tags or tag soup and other malformed markup). \n",
      "\n",
      "In this tutorial, we will collect and parse a web page in order to grab textual data and write the information we have gathered to a CSV file. \n",
      "\n",
      "Before working on this tutorial, you should have a local or server-based Python programming environment set up on your machine.\n",
      "\n",
      "You should have the Requests and Beautiful Soup modules installed, which you can achieve by following our tutorial “How To Work with Web Data Using Requests and Beautiful Soup with Python 3.” It would also be useful to have a working familiarity with these modules. \n",
      "\n",
      "Additionally, since we will be working with data scraped from the web, you should be comfortable with HTML structure and tagging. \n",
      "\n",
      "In this tutorial, we’ll be working with data from the official website of the National Gallery of Art in the United States. The National Gallery is an art museum located on the National Mall in Washington, D.C. It holds over 120,000 pieces dated from the Renaissance to the present day done by more than 13,000 artists.\n",
      "\n",
      "We would like to search the Index of Artists, which is available at https://www.nga.gov/collection/an.shtm. \n",
      "\n",
      "\n",
      "\n",
      "Since we’ll be doing this project in order to learn about web scraping with Beautiful Soup, we don’t need to pull too much data from the site, so let’s limit the scope of the artist data we are looking to scrape. Let’s therefore choose one letter — in our example we’ll choose the letter Z — and we’ll see a page that looks like this:\n",
      "\n",
      "\n",
      "\n",
      "In the page above, we see that the first artist listed at the time of writing is Zabaglia, Niccola, which is a good thing to note for when we start pulling data. We’ll start by working with this first page, with the following URL for the letter Z:\n",
      "\n",
      "It is important to note for later how many pages total there are for the letter you are choosing to list, which you can discover by clicking through to the last page of artists. In this case, there are 5 pages total, and the last artist listed at the time of writing is Zykmund, Václav. The last page of Z artists has the following URL:\n",
      "\n",
      "To begin to familiarize yourself with the DOM of this web page, you can open your browser’s Developer Tools.\n",
      "\n",
      "To begin our coding project, let’s activate our Python 3 programming environment. Make sure you’re in the directory where your environment is located, and run the following command:\n",
      "\n",
      "With our programming environment activated, we’ll create a new file, with nano for instance. You can name your file whatever you would like, we’ll call it nga_z_artists.py in this tutorial.\n",
      "\n",
      "Within this file, we can begin to import the libraries we’ll be using — Requests and Beautiful Soup.\n",
      "\n",
      "The Requests library allows you to make use of HTTP within your Python programs in a human readable way, and the Beautiful Soup module is designed to get web scraping done quickly.\n",
      "\n",
      "We will import both Requests and Beautiful Soup with the import statement. For Beautiful Soup, we’ll be importing it from bs4, the package in which Beautiful Soup 4 is found. \n",
      "\n",
      "With both the Requests and Beautiful Soup modules imported, we can move on to working to first collect a page and then parse it. \n",
      "\n",
      "The next step we will need to do is collect the URL of the first web page with Requests. We’ll assign the URL for the first page to the variable page by using the method requests.get().\n",
      "\n",
      "Next, we’ll create a BeautifulSoup object, or a parse tree. This object takes as its arguments the page.text document from Requests (the content of the server’s response) and then parses it from Python’s built-in html.parser.\n",
      "\n",
      "Now with our page collected, parsed, and set up as a BeautifulSoup object, we can move on to collecting the data that we would like.\n",
      "\n",
      "For this project, we’ll collect artists’ names and the relevant links available on the website. You may want to collect different data, such as the artists’ nationality and dates. Whatever data you would like to collect, you need to find out how it is described by the DOM of the web page. \n",
      "\n",
      "To do this, in your web browser, right-click — or CTRL + click on macOS — on the first artist’s name, Zabaglia, Niccola. Within the context menu that pops up, you should see a menu item similar to Inspect Element (Firefox) or Inspect (Chrome).\n",
      "\n",
      "\n",
      "\n",
      "Once you click on the relevant Inspect menu item, the tools for web developers should appear within your browser. We want to look for the class and tags associated with the artists’ names in this list. \n",
      "\n",
      "\n",
      "\n",
      "We’ll see first that the table of names is within <div> tags where class=\"BodyText\". This is important to note so that we only search for text within this section of the web page. We also notice that the name Zabaglia, Niccola is in a link tag, since the name references a web page that describes the artist. So we will want to reference the <a> tag for links. Each artist’s name is a reference to a link. \n",
      "\n",
      "To do this, we’ll use Beautiful Soup’s find() and find_all() methods in order to pull the text of the artists’ names from the BodyText <div>. \n",
      "\n",
      "Next, at the bottom of our program file, we will want to create a for loop in order to iterate over all the artist names that we just put into the artist_name_list_items variable. \n",
      "\n",
      "We’ll print these names out with the prettify() method in order to turn the Beautiful Soup parse tree into a nicely formatted Unicode string.\n",
      "\n",
      "Let’s run the program as we have it so far:\n",
      "\n",
      "Once we do so, we’ll receive the following output:\n",
      "\n",
      "What we see in the output at this point is the full text and tags related to all of the artists’ names within the <a> tags found in the <div class=\"BodyText\"> tag on the first page, as well as some additional link text at the bottom. Since we don’t want this extra information, let’s work on removing this in the next section.\n",
      "\n",
      "So far, we have been able to collect all the link text data within one <div> section of our web page. However, we don’t want to have the bottom links that don’t reference artists’ names, so let’s work to remove that part. \n",
      "\n",
      "In order to remove the bottom links of the page, let’s again right-click and Inspect the DOM. We’ll see that the links on the bottom of the <div class=\"BodyText\"> section are contained in an HTML table: <table class=\"AlphaNav\">:\n",
      "\n",
      "\n",
      "\n",
      "We can therefore use Beautiful Soup to find the AlphaNav class and use the decompose() method to remove a tag from the parse tree and then destroy it along with its contents. \n",
      "\n",
      "We’ll use the variable last_links to reference these bottom links and add them to the program file:\n",
      "\n",
      "Now, when we run the program with the python nga_z_artist.py command, we’ll receive the following output:\n",
      "\n",
      "At this point, we see that the output no longer includes the links at the bottom of the web page, and now only displays the links associated with artists’ names. \n",
      "\n",
      "So far we have targeted the links with the artists’ names specifically, but we have the extra tag data that we don’t really want. Let’s remove that in the next section.\n",
      "\n",
      "In order to access only the actual artists’ names, we’ll want to target the contents of the <a> tags rather than print out the entire link tag.\n",
      "\n",
      "We can do this with Beautiful Soup’s .contents, which will return the tag’s children as a Python list data type.\n",
      "\n",
      "Let’s revise the for loop so that instead of printing the entire link and its tag, we’ll print the list of children (i.e. the artists’ full names):\n",
      "\n",
      "Note that we are iterating over the list above by calling on the index number of each item.\n",
      "\n",
      "We can run the program with the python command to view the following output:\n",
      "\n",
      "We have received back a list of all the artists’ names available on the first page of the letter Z. \n",
      "\n",
      "However, what if we want to also capture the URLs associated with those artists? We can extract URLs found within a page’s <a> tags by using Beautiful Soup’s get('href') method. \n",
      "\n",
      "From the output of the links above, we know that the entire URL is not being captured, so we will concatenate the link string with the front of the URL string (in this case https://www.nga.gov).\n",
      "\n",
      "These lines we’ll also add to the for loop:\n",
      "\n",
      "When we run the program above, we’ll receive both the artists’ names and the URLs to the links that tell us more about the artists:\n",
      "\n",
      "Although we are now getting information from the website, it is currently just printing to our terminal window. Let’s instead capture this data so that we can use it elsewhere by writing it to a file.\n",
      "\n",
      "Collecting data that only lives in a terminal window is not very useful. Comma-separated values (CSV) files allows us to store tabular data in plain text, and is a common format for spreadsheets and databases. Before beginning with this section, you should familiarize yourself with how to handle plain text files in Python.\n",
      "\n",
      "First, we need to import Python’s built-in csv module along with the other modules at the top of the Python programming file:\n",
      "\n",
      "Next, we’ll create and open a file called z-artist-names.csv for us to write to (we’ll use the variable f for file here) by using the 'w' mode. We’ll also write the top row headings: Name and Link which we’ll pass to the writerow() method as a list:\n",
      "\n",
      "Finally, within our for loop, we’ll write each row with the artists’ names and their associated links:\n",
      "\n",
      "You can see the lines for each of these tasks in the file below:\n",
      "\n",
      "When you run the program now with the python command, no output will be returned to your terminal window. Instead, a file will be created in the directory you are working in called z-artist-names.csv. \n",
      "\n",
      "Depending on what you use to open it, it may look something like this:\n",
      "\n",
      "Or, it may look more like a spreadsheet:\n",
      "\n",
      "\n",
      "\n",
      "In either case, you can now use this file to work with the data in more meaningful ways since the information you have collected is now stored in your computer’s memory.\n",
      "\n",
      "We have created a program that will pull data from the first page of the list of artists whose last names start with the letter Z. However, there are 5 pages in total of these artists available on the website.\n",
      "\n",
      "In order to collect all of these pages, we can perform more iterations with for loops. This will revise most of the code we have written so far, but will employ similar concepts.\n",
      "\n",
      "To start, we’ll want to initialize a list to hold the pages:\n",
      "\n",
      "We will populate this initialized list with the following for loop:\n",
      "\n",
      "Earlier in this tutorial, we noted that we should pay attention to the total number of pages there are that contain artists’ names starting with the letter Z (or whatever letter we’re using). Since there are 5 pages for the letter Z, we constructed the for loop above with a range of 1 to 6 so that it will iterate through each of the 5 pages. \n",
      "\n",
      "For this specific web site, the URLs begin with the string https://www.nga.gov/collection/anZ and then are followed with a number for the page (which will be the integer i from the for loop that we convert to a string) and end with .htm. We will concatenate these strings together and then append the result to the pages list.\n",
      "\n",
      "In addition to this loop, we’ll have a second loop that will go through each of the pages above. The code in this for loop will look similar to the code we have created so far, as it is doing the task we completed for the first page of the letter Z artists for each of the 5 pages total. Note that because we have put the original program into the second for loop, we now have the original loop as a nested for loop contained in it. \n",
      "\n",
      "The two for loops will look like this:\n",
      "\n",
      "In the code above, you should see that the first for loop is iterating over the pages and the second for loop is scraping data from each of those pages and then is adding the artists’ names and links line by line through each row of each page. \n",
      "\n",
      "These two for loops come below the import statements, the CSV file creation and writer (with the line for writing the headers of the file), and the initialization of the pages variable (assigned to a list).\n",
      "\n",
      "Within the greater context of the programming file, the complete code looks like this:\n",
      "\n",
      "Since this program is doing a bit of work, it will take a little while to create the CSV file. Once it is done, the output will be complete, showing the artists’ names and their associated links from Zabaglia, Niccola to Zykmund, Václav.\n",
      "\n",
      "When scraping web pages, it is important to remain considerate of the servers you are grabbing information from. \n",
      "\n",
      "Check to see if a site has terms of service or terms of use that pertains to web scraping. Also, check to see if a site has an API that allows you to grab data before scraping it yourself. \n",
      "\n",
      "Be sure to not continuously hit servers to gather data. Once you have collected what you need from a site, run scripts that will go over the data locally rather than burden someone else’s servers. \n",
      "\n",
      "Additionally, it is a good idea to scrape with a header that has your name and email so that a website can identify you and follow up if they have any questions. An example of a header you can use with the Python Requests library is as follows:\n",
      "\n",
      "Using headers with identifiable information ensures that the people who go over a server’s logs can reach out to you.\n",
      "\n",
      "This tutorial went through using Python and Beautiful Soup to scrape data from a website. We stored the text that we gathered within a CSV file.\n",
      "\n",
      "You can continue working on this project by collecting more data and making your CSV file more robust. For example, you may want to include the nationalities and years of each artist. You can also use what you have learned to scrape data from other websites. \n",
      "\n",
      "To continue learning about pulling information from the web, read our tutorial “How To Crawl A Web Page with Scrapy and Python 3.”\n",
      "\n",
      "Simple setup. Full root access. Straightforward pricing.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = \"https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3\"\n",
    "\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "data=''\n",
    "for tag in soup.find_all('p'):\n",
    "    data=data+re.sub(r\"\\[.*?\\]\", \"\",tag.text)+tag.next_sibling+\"\\n\"\n",
    "    \n",
    "print(data) \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Get the latest tutorials on SysAdmin and open source topics.\\n  \\n\\nMany data analysis, big data, and machine learning projects require scraping websites to gather the data that you’ll be working with. The Python programming language is widely used in the data science community, and therefore has an ecosystem of modules and tools that you can use in your own projects. In this tutorial we will be focusing on the Beautiful Soup module. \\n\\nBeautiful Soup, an allusion to the Mock Turtle’s song found in Chapter 10 of Lewis Carroll’s Alice’s Adventures in Wonderland, is a Python library that allows for quick turnaround on web scraping projects. Currently available as Beautiful Soup 4 and compatible with both Python 2.7 and Python 3, Beautiful Soup creates a parse tree from parsed HTML and XML documents (including documents with non-closed tags or tag soup and other malformed markup). \\n\\nIn this tutorial, we will collect and parse a web page in order to grab textual data and write the information we have gathered to a CSV file. \\n\\nBefore working on this tutorial, you should have a local or server-based Python programming environment set up on your machine.\\n\\nYou should have the Requests and Beautiful Soup modules installed, which you can achieve by following our tutorial “How To Work with Web Data Using Requests and Beautiful Soup with Python 3.” It would also be useful to have a working familiarity with these modules. \\n\\nAdditionally, since we will be working with data scraped from the web, you should be comfortable with HTML structure and tagging. \\n\\nIn this tutorial, we’ll be working with data from the official website of the National Gallery of Art in the United States. The National Gallery is an art museum located on the National Mall in Washington, D.C. It holds over 120,000 pieces dated from the Renaissance to the present day done by more than 13,000 artists.\\n\\nWe would like to search the Index of Artists, which is available at https://www.nga.gov/collection/an.shtm. \\n\\n\\n\\nSince we’ll be doing this project in order to learn about web scraping with Beautiful Soup, we don’t need to pull too much data from the site, so let’s limit the scope of the artist data we are looking to scrape. Let’s therefore choose one letter — in our example we’ll choose the letter Z — and we’ll see a page that looks like this:\\n\\n\\n\\nIn the page above, we see that the first artist listed at the time of writing is Zabaglia, Niccola, which is a good thing to note for when we start pulling data. We’ll start by working with this first page, with the following URL for the letter Z:\\n\\nIt is important to note for later how many pages total there are for the letter you are choosing to list, which you can discover by clicking through to the last page of artists. In this case, there are 5 pages total, and the last artist listed at the time of writing is Zykmund, Václav. The last page of Z artists has the following URL:\\n\\nTo begin to familiarize yourself with the DOM of this web page, you can open your browser’s Developer Tools.\\n\\nTo begin our coding project, let’s activate our Python 3 programming environment. Make sure you’re in the directory where your environment is located, and run the following command:\\n\\nWith our programming environment activated, we’ll create a new file, with nano for instance. You can name your file whatever you would like, we’ll call it nga_z_artists.py in this tutorial.\\n\\nWithin this file, we can begin to import the libraries we’ll be using — Requests and Beautiful Soup.\\n\\nThe Requests library allows you to make use of HTTP within your Python programs in a human readable way, and the Beautiful Soup module is designed to get web scraping done quickly.\\n\\nWe will import both Requests and Beautiful Soup with the import statement. For Beautiful Soup, we’ll be importing it from bs4, the package in which Beautiful Soup 4 is found. \\n\\nWith both the Requests and Beautiful Soup modules imported, we can move on to working to first collect a page and then parse it. \\n\\nThe next step we will need to do is collect the URL of the first web page with Requests. We’ll assign the URL for the first page to the variable page by using the method requests.get().\\n\\nNext, we’ll create a BeautifulSoup object, or a parse tree. This object takes as its arguments the page.text document from Requests (the content of the server’s response) and then parses it from Python’s built-in html.parser.\\n\\nNow with our page collected, parsed, and set up as a BeautifulSoup object, we can move on to collecting the data that we would like.\\n\\nFor this project, we’ll collect artists’ names and the relevant links available on the website. You may want to collect different data, such as the artists’ nationality and dates. Whatever data you would like to collect, you need to find out how it is described by the DOM of the web page. \\n\\nTo do this, in your web browser, right-click — or CTRL + click on macOS — on the first artist’s name, Zabaglia, Niccola. Within the context menu that pops up, you should see a menu item similar to Inspect Element (Firefox) or Inspect (Chrome).\\n\\n\\n\\nOnce you click on the relevant Inspect menu item, the tools for web developers should appear within your browser. We want to look for the class and tags associated with the artists’ names in this list. \\n\\n\\n\\nWe’ll see first that the table of names is within <div> tags where class=\"BodyText\". This is important to note so that we only search for text within this section of the web page. We also notice that the name Zabaglia, Niccola is in a link tag, since the name references a web page that describes the artist. So we will want to reference the <a> tag for links. Each artist’s name is a reference to a link. \\n\\nTo do this, we’ll use Beautiful Soup’s find() and find_all() methods in order to pull the text of the artists’ names from the BodyText <div>. \\n\\nNext, at the bottom of our program file, we will want to create a for loop in order to iterate over all the artist names that we just put into the artist_name_list_items variable. \\n\\nWe’ll print these names out with the prettify() method in order to turn the Beautiful Soup parse tree into a nicely formatted Unicode string.\\n\\nLet’s run the program as we have it so far:\\n\\nOnce we do so, we’ll receive the following output:\\n\\nWhat we see in the output at this point is the full text and tags related to all of the artists’ names within the <a> tags found in the <div class=\"BodyText\"> tag on the first page, as well as some additional link text at the bottom. Since we don’t want this extra information, let’s work on removing this in the next section.\\n\\nSo far, we have been able to collect all the link text data within one <div> section of our web page. However, we don’t want to have the bottom links that don’t reference artists’ names, so let’s work to remove that part. \\n\\nIn order to remove the bottom links of the page, let’s again right-click and Inspect the DOM. We’ll see that the links on the bottom of the <div class=\"BodyText\"> section are contained in an HTML table: <table class=\"AlphaNav\">:\\n\\n\\n\\nWe can therefore use Beautiful Soup to find the AlphaNav class and use the decompose() method to remove a tag from the parse tree and then destroy it along with its contents. \\n\\nWe’ll use the variable last_links to reference these bottom links and add them to the program file:\\n\\nNow, when we run the program with the python nga_z_artist.py command, we’ll receive the following output:\\n\\nAt this point, we see that the output no longer includes the links at the bottom of the web page, and now only displays the links associated with artists’ names. \\n\\nSo far we have targeted the links with the artists’ names specifically, but we have the extra tag data that we don’t really want. Let’s remove that in the next section.\\n\\nIn order to access only the actual artists’ names, we’ll want to target the contents of the <a> tags rather than print out the entire link tag.\\n\\nWe can do this with Beautiful Soup’s .contents, which will return the tag’s children as a Python list data type.\\n\\nLet’s revise the for loop so that instead of printing the entire link and its tag, we’ll print the list of children (i.e. the artists’ full names):\\n\\nNote that we are iterating over the list above by calling on the index number of each item.\\n\\nWe can run the program with the python command to view the following output:\\n\\nWe have received back a list of all the artists’ names available on the first page of the letter Z. \\n\\nHowever, what if we want to also capture the URLs associated with those artists? We can extract URLs found within a page’s <a> tags by using Beautiful Soup’s get(\\'href\\') method. \\n\\nFrom the output of the links above, we know that the entire URL is not being captured, so we will concatenate the link string with the front of the URL string (in this case https://www.nga.gov).\\n\\nThese lines we’ll also add to the for loop:\\n\\nWhen we run the program above, we’ll receive both the artists’ names and the URLs to the links that tell us more about the artists:\\n\\nAlthough we are now getting information from the website, it is currently just printing to our terminal window. Let’s instead capture this data so that we can use it elsewhere by writing it to a file.\\n\\nCollecting data that only lives in a terminal window is not very useful. Comma-separated values (CSV) files allows us to store tabular data in plain text, and is a common format for spreadsheets and databases. Before beginning with this section, you should familiarize yourself with how to handle plain text files in Python.\\n\\nFirst, we need to import Python’s built-in csv module along with the other modules at the top of the Python programming file:\\n\\nNext, we’ll create and open a file called z-artist-names.csv for us to write to (we’ll use the variable f for file here) by using the \\'w\\' mode. We’ll also write the top row headings: Name and Link which we’ll pass to the writerow() method as a list:\\n\\nFinally, within our for loop, we’ll write each row with the artists’ names and their associated links:\\n\\nYou can see the lines for each of these tasks in the file below:\\n\\nWhen you run the program now with the python command, no output will be returned to your terminal window. Instead, a file will be created in the directory you are working in called z-artist-names.csv. \\n\\nDepending on what you use to open it, it may look something like this:\\n\\nOr, it may look more like a spreadsheet:\\n\\n\\n\\nIn either case, you can now use this file to work with the data in more meaningful ways since the information you have collected is now stored in your computer’s memory.\\n\\nWe have created a program that will pull data from the first page of the list of artists whose last names start with the letter Z. However, there are 5 pages in total of these artists available on the website.\\n\\nIn order to collect all of these pages, we can perform more iterations with for loops. This will revise most of the code we have written so far, but will employ similar concepts.\\n\\nTo start, we’ll want to initialize a list to hold the pages:\\n\\nWe will populate this initialized list with the following for loop:\\n\\nEarlier in this tutorial, we noted that we should pay attention to the total number of pages there are that contain artists’ names starting with the letter Z (or whatever letter we’re using). Since there are 5 pages for the letter Z, we constructed the for loop above with a range of 1 to 6 so that it will iterate through each of the 5 pages. \\n\\nFor this specific web site, the URLs begin with the string https://www.nga.gov/collection/anZ and then are followed with a number for the page (which will be the integer i from the for loop that we convert to a string) and end with .htm. We will concatenate these strings together and then append the result to the pages list.\\n\\nIn addition to this loop, we’ll have a second loop that will go through each of the pages above. The code in this for loop will look similar to the code we have created so far, as it is doing the task we completed for the first page of the letter Z artists for each of the 5 pages total. Note that because we have put the original program into the second for loop, we now have the original loop as a nested for loop contained in it. \\n\\nThe two for loops will look like this:\\n\\nIn the code above, you should see that the first for loop is iterating over the pages and the second for loop is scraping data from each of those pages and then is adding the artists’ names and links line by line through each row of each page. \\n\\nThese two for loops come below the import statements, the CSV file creation and writer (with the line for writing the headers of the file), and the initialization of the pages variable (assigned to a list).\\n\\nWithin the greater context of the programming file, the complete code looks like this:\\n\\nSince this program is doing a bit of work, it will take a little while to create the CSV file. Once it is done, the output will be complete, showing the artists’ names and their associated links from Zabaglia, Niccola to Zykmund, Václav.\\n\\nWhen scraping web pages, it is important to remain considerate of the servers you are grabbing information from. \\n\\nCheck to see if a site has terms of service or terms of use that pertains to web scraping. Also, check to see if a site has an API that allows you to grab data before scraping it yourself. \\n\\nBe sure to not continuously hit servers to gather data. Once you have collected what you need from a site, run scripts that will go over the data locally rather than burden someone else’s servers. \\n\\nAdditionally, it is a good idea to scrape with a header that has your name and email so that a website can identify you and follow up if they have any questions. An example of a header you can use with the Python Requests library is as follows:\\n\\nUsing headers with identifiable information ensures that the people who go over a server’s logs can reach out to you.\\n\\nThis tutorial went through using Python and Beautiful Soup to scrape data from a website. We stored the text that we gathered within a CSV file.\\n\\nYou can continue working on this project by collecting more data and making your CSV file more robust. For example, you may want to include the nationalities and years of each artist. You can also use what you have learned to scrape data from other websites. \\n\\nTo continue learning about pulling information from the web, read our tutorial “How To Crawl A Web Page with Scrapy and Python 3.”\\n\\nSimple setup. Full root access. Straightforward pricing.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Get the latest tutorials on SysAdmin and open source topics.\\n  \\n\\nMany data analysis, big data, and machine learning projects require scraping websites to gather the data that you’ll be working with. The Python programming language is widely used in the data science community, and therefore has an ecosystem of modules and tools that you can use in your own projects. In this tutorial we will be focusing on the Beautiful Soup module. \\n\\nBeautiful Soup, an allusion to the Mock Turtle’s song found in Chapter 10 of Lewis Carroll’s Alice’s Adventures in Wonderland, is a Python library that allows for quick turnaround on web scraping projects. Currently available as Beautiful Soup 4 and compatible with both Python 2.7 and Python 3, Beautiful Soup creates a parse tree from parsed HTML and XML documents (including documents with non-closed tags or tag soup and other malformed markup). \\n\\nIn this tutorial, we will collect and parse a web page in order to grab textual data and write the information we have gathered to a CSV file. \\n\\nBefore working on this tutorial, you should have a local or server-based Python programming environment set up on your machine.\\n\\nYou should have the Requests and Beautiful Soup modules installed, which you can achieve by following our tutorial “How To Work with Web Data Using Requests and Beautiful Soup with Python 3.” It would also be useful to have a working familiarity with these modules. \\n\\nAdditionally, since we will be working with data scraped from the web, you should be comfortable with HTML structure and tagging. \\n\\nIn this tutorial, we’ll be working with data from the official website of the National Gallery of Art in the United States. The National Gallery is an art museum located on the National Mall in Washington, D.C. It holds over 120,000 pieces dated from the Renaissance to the present day done by more than 13,000 artists.\\n\\nWe would like to search the Index of Artists, which is available at https://www.nga.gov/collection/an.shtm. \\n\\n\\n\\nSince we’ll be doing this project in order to learn about web scraping with Beautiful Soup, we don’t need to pull too much data from the site, so let’s limit the scope of the artist data we are looking to scrape. Let’s therefore choose one letter — in our example we’ll choose the letter Z — and we’ll see a page that looks like this:\\n\\n\\n\\nIn the page above, we see that the first artist listed at the time of writing is Zabaglia, Niccola, which is a good thing to note for when we start pulling data. We’ll start by working with this first page, with the following URL for the letter Z:\\n\\nIt is important to note for later how many pages total there are for the letter you are choosing to list, which you can discover by clicking through to the last page of artists. In this case, there are 5 pages total, and the last artist listed at the time of writing is Zykmund, Václav. The last page of Z artists has the following URL:\\n\\nTo begin to familiarize yourself with the DOM of this web page, you can open your browser’s Developer Tools.\\n\\nTo begin our coding project, let’s activate our Python 3 programming environment. Make sure you’re in the directory where your environment is located, and run the following command:\\n\\nWith our programming environment activated, we’ll create a new file, with nano for instance. You can name your file whatever you would like, we’ll call it nga_z_artists.py in this tutorial.\\n\\nWithin this file, we can begin to import the libraries we’ll be using — Requests and Beautiful Soup.\\n\\nThe Requests library allows you to make use of HTTP within your Python programs in a human readable way, and the Beautiful Soup module is designed to get web scraping done quickly.\\n\\nWe will import both Requests and Beautiful Soup with the import statement. For Beautiful Soup, we’ll be importing it from bs4, the package in which Beautiful Soup 4 is found. \\n\\nWith both the Requests and Beautiful Soup modules imported, we can move on to working to first collect a page and then parse it. \\n\\nThe next step we will need to do is collect the URL of the first web page with Requests. We’ll assign the URL for the first page to the variable page by using the method requests.get().\\n\\nNext, we’ll create a BeautifulSoup object, or a parse tree. This object takes as its arguments the page.text document from Requests (the content of the server’s response) and then parses it from Python’s built-in html.parser.\\n\\nNow with our page collected, parsed, and set up as a BeautifulSoup object, we can move on to collecting the data that we would like.\\n\\nFor this project, we’ll collect artists’ names and the relevant links available on the website. You may want to collect different data, such as the artists’ nationality and dates. Whatever data you would like to collect, you need to find out how it is described by the DOM of the web page. \\n\\nTo do this, in your web browser, right-click — or CTRL + click on macOS — on the first artist’s name, Zabaglia, Niccola. Within the context menu that pops up, you should see a menu item similar to Inspect Element (Firefox) or Inspect (Chrome).\\n\\n\\n\\nOnce you click on the relevant Inspect menu item, the tools for web developers should appear within your browser. We want to look for the class and tags associated with the artists’ names in this list. \\n\\n\\n\\nWe’ll see first that the table of names is within <div> tags where class=\"BodyText\". This is important to note so that we only search for text within this section of the web page. We also notice that the name Zabaglia, Niccola is in a link tag, since the name references a web page that describes the artist. So we will want to reference the <a> tag for links. Each artist’s name is a reference to a link. \\n\\nTo do this, we’ll use Beautiful Soup’s find() and find_all() methods in order to pull the text of the artists’ names from the BodyText <div>. \\n\\nNext, at the bottom of our program file, we will want to create a for loop in order to iterate over all the artist names that we just put into the artist_name_list_items variable. \\n\\nWe’ll print these names out with the prettify() method in order to turn the Beautiful Soup parse tree into a nicely formatted Unicode string.\\n\\nLet’s run the program as we have it so far:\\n\\nOnce we do so, we’ll receive the following output:\\n\\nWhat we see in the output at this point is the full text and tags related to all of the artists’ names within the <a> tags found in the <div class=\"BodyText\"> tag on the first page, as well as some additional link text at the bottom. Since we don’t want this extra information, let’s work on removing this in the next section.\\n\\nSo far, we have been able to collect all the link text data within one <div> section of our web page. However, we don’t want to have the bottom links that don’t reference artists’ names, so let’s work to remove that part. \\n\\nIn order to remove the bottom links of the page, let’s again right-click and Inspect the DOM. We’ll see that the links on the bottom of the <div class=\"BodyText\"> section are contained in an HTML table: <table class=\"AlphaNav\">:\\n\\n\\n\\nWe can therefore use Beautiful Soup to find the AlphaNav class and use the decompose() method to remove a tag from the parse tree and then destroy it along with its contents. \\n\\nWe’ll use the variable last_links to reference these bottom links and add them to the program file:\\n\\nNow, when we run the program with the python nga_z_artist.py command, we’ll receive the following output:\\n\\nAt this point, we see that the output no longer includes the links at the bottom of the web page, and now only displays the links associated with artists’ names. \\n\\nSo far we have targeted the links with the artists’ names specifically, but we have the extra tag data that we don’t really want. Let’s remove that in the next section.\\n\\nIn order to access only the actual artists’ names, we’ll want to target the contents of the <a> tags rather than print out the entire link tag.\\n\\nWe can do this with Beautiful Soup’s .contents, which will return the tag’s children as a Python list data type.\\n\\nLet’s revise the for loop so that instead of printing the entire link and its tag, we’ll print the list of children (i.e. the artists’ full names):\\n\\nNote that we are iterating over the list above by calling on the index number of each item.\\n\\nWe can run the program with the python command to view the following output:\\n\\nWe have received back a list of all the artists’ names available on the first page of the letter Z. \\n\\nHowever, what if we want to also capture the URLs associated with those artists? We can extract URLs found within a page’s <a> tags by using Beautiful Soup’s get(\\'href\\') method. \\n\\nFrom the output of the links above, we know that the entire URL is not being captured, so we will concatenate the link string with the front of the URL string (in this case https://www.nga.gov).\\n\\nThese lines we’ll also add to the for loop:\\n\\nWhen we run the program above, we’ll receive both the artists’ names and the URLs to the links that tell us more about the artists:\\n\\nAlthough we are now getting information from the website, it is currently just printing to our terminal window. Let’s instead capture this data so that we can use it elsewhere by writing it to a file.\\n\\nCollecting data that only lives in a terminal window is not very useful. Comma-separated values (CSV) files allows us to store tabular data in plain text, and is a common format for spreadsheets and databases. Before beginning with this section, you should familiarize yourself with how to handle plain text files in Python.\\n\\nFirst, we need to import Python’s built-in csv module along with the other modules at the top of the Python programming file:\\n\\nNext, we’ll create and open a file called z-artist-names.csv for us to write to (we’ll use the variable f for file here) by using the \\'w\\' mode. We’ll also write the top row headings: Name and Link which we’ll pass to the writerow() method as a list:\\n\\nFinally, within our for loop, we’ll write each row with the artists’ names and their associated links:\\n\\nYou can see the lines for each of these tasks in the file below:\\n\\nWhen you run the program now with the python command, no output will be returned to your terminal window. Instead, a file will be created in the directory you are working in called z-artist-names.csv. \\n\\nDepending on what you use to open it, it may look something like this:\\n\\nOr, it may look more like a spreadsheet:\\n\\n\\n\\nIn either case, you can now use this file to work with the data in more meaningful ways since the information you have collected is now stored in your computer’s memory.\\n\\nWe have created a program that will pull data from the first page of the list of artists whose last names start with the letter Z. However, there are 5 pages in total of these artists available on the website.\\n\\nIn order to collect all of these pages, we can perform more iterations with for loops. This will revise most of the code we have written so far, but will employ similar concepts.\\n\\nTo start, we’ll want to initialize a list to hold the pages:\\n\\nWe will populate this initialized list with the following for loop:\\n\\nEarlier in this tutorial, we noted that we should pay attention to the total number of pages there are that contain artists’ names starting with the letter Z (or whatever letter we’re using). Since there are 5 pages for the letter Z, we constructed the for loop above with a range of 1 to 6 so that it will iterate through each of the 5 pages. \\n\\nFor this specific web site, the URLs begin with the string https://www.nga.gov/collection/anZ and then are followed with a number for the page (which will be the integer i from the for loop that we convert to a string) and end with .htm. We will concatenate these strings together and then append the result to the pages list.\\n\\nIn addition to this loop, we’ll have a second loop that will go through each of the pages above. The code in this for loop will look similar to the code we have created so far, as it is doing the task we completed for the first page of the letter Z artists for each of the 5 pages total. Note that because we have put the original program into the second for loop, we now have the original loop as a nested for loop contained in it. \\n\\nThe two for loops will look like this:\\n\\nIn the code above, you should see that the first for loop is iterating over the pages and the second for loop is scraping data from each of those pages and then is adding the artists’ names and links line by line through each row of each page. \\n\\nThese two for loops come below the import statements, the CSV file creation and writer (with the line for writing the headers of the file), and the initialization of the pages variable (assigned to a list).\\n\\nWithin the greater context of the programming file, the complete code looks like this:\\n\\nSince this program is doing a bit of work, it will take a little while to create the CSV file. Once it is done, the output will be complete, showing the artists’ names and their associated links from Zabaglia, Niccola to Zykmund, Václav.\\n\\nWhen scraping web pages, it is important to remain considerate of the servers you are grabbing information from. \\n\\nCheck to see if a site has terms of service or terms of use that pertains to web scraping. Also, check to see if a site has an API that allows you to grab data before scraping it yourself. \\n\\nBe sure to not continuously hit servers to gather data. Once you have collected what you need from a site, run scripts that will go over the data locally rather than burden someone else’s servers. \\n\\nAdditionally, it is a good idea to scrape with a header that has your name and email so that a website can identify you and follow up if they have any questions. An example of a header you can use with the Python Requests library is as follows:\\n\\nUsing headers with identifiable information ensures that the people who go over a server’s logs can reach out to you.\\n\\nThis tutorial went through using Python and Beautiful Soup to scrape data from a website. We stored the text that we gathered within a CSV file.\\n\\nYou can continue working on this project by collecting more data and making your CSV file more robust. For example, you may want to include the nationalities and years of each artist. You can also use what you have learned to scrape data from other websites. \\n\\nTo continue learning about pulling information from the web, read our tutorial “How To Crawl A Web Page with Scrapy and Python 3.”\\n\\nSimple setup. Full root access. Straightforward pricing.\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Get the latest tutorials on SysAdmin and open source topics.  Many data analysis, big data, and machine learning projects require scraping websites to gather the data that you’ll be working with. The Python programming language is widely used in the data science community, and therefore has an ecosystem of modules and tools that you can use in your own projects. In this tutorial we will be focusing on the Beautiful Soup module. Beautiful Soup, an allusion to the Mock Turtle’s song found in Chapter 10 of Lewis Carroll’s Alice’s Adventures in Wonderland, is a Python library that allows for quick turnaround on web scraping projects. Currently available as Beautiful Soup 4 and compatible with both Python 2.7 and Python 3, Beautiful Soup creates a parse tree from parsed HTML and XML documents (including documents with non-closed tags or tag soup and other malformed markup). In this tutorial, we will collect and parse a web page in order to grab textual data and write the information we have gathered to a CSV file. Before working on this tutorial, you should have a local or server-based Python programming environment set up on your machine.You should have the Requests and Beautiful Soup modules installed, which you can achieve by following our tutorial “How To Work with Web Data Using Requests and Beautiful Soup with Python 3.” It would also be useful to have a working familiarity with these modules. Additionally, since we will be working with data scraped from the web, you should be comfortable with HTML structure and tagging. In this tutorial, we’ll be working with data from the official website of the National Gallery of Art in the United States. The National Gallery is an art museum located on the National Mall in Washington, D.C. It holds over 120,000 pieces dated from the Renaissance to the present day done by more than 13,000 artists.We would like to search the Index of Artists, which is available at https://www.nga.gov/collection/an.shtm. Since we’ll be doing this project in order to learn about web scraping with Beautiful Soup, we don’t need to pull too much data from the site, so let’s limit the scope of the artist data we are looking to scrape. Let’s therefore choose one letter — in our example we’ll choose the letter Z — and we’ll see a page that looks like this:In the page above, we see that the first artist listed at the time of writing is Zabaglia, Niccola, which is a good thing to note for when we start pulling data. We’ll start by working with this first page, with the following URL for the letter Z:It is important to note for later how many pages total there are for the letter you are choosing to list, which you can discover by clicking through to the last page of artists. In this case, there are 5 pages total, and the last artist listed at the time of writing is Zykmund, Václav. The last page of Z artists has the following URL:To begin to familiarize yourself with the DOM of this web page, you can open your browser’s Developer Tools.To begin our coding project, let’s activate our Python 3 programming environment. Make sure you’re in the directory where your environment is located, and run the following command:With our programming environment activated, we’ll create a new file, with nano for instance. You can name your file whatever you would like, we’ll call it nga_z_artists.py in this tutorial.Within this file, we can begin to import the libraries we’ll be using — Requests and Beautiful Soup.The Requests library allows you to make use of HTTP within your Python programs in a human readable way, and the Beautiful Soup module is designed to get web scraping done quickly.We will import both Requests and Beautiful Soup with the import statement. For Beautiful Soup, we’ll be importing it from bs4, the package in which Beautiful Soup 4 is found. With both the Requests and Beautiful Soup modules imported, we can move on to working to first collect a page and then parse it. The next step we will need to do is collect the URL of the first web page with Requests. We’ll assign the URL for the first page to the variable page by using the method requests.get().Next, we’ll create a BeautifulSoup object, or a parse tree. This object takes as its arguments the page.text document from Requests (the content of the server’s response) and then parses it from Python’s built-in html.parser.Now with our page collected, parsed, and set up as a BeautifulSoup object, we can move on to collecting the data that we would like.For this project, we’ll collect artists’ names and the relevant links available on the website. You may want to collect different data, such as the artists’ nationality and dates. Whatever data you would like to collect, you need to find out how it is described by the DOM of the web page. To do this, in your web browser, right-click — or CTRL + click on macOS — on the first artist’s name, Zabaglia, Niccola. Within the context menu that pops up, you should see a menu item similar to Inspect Element (Firefox) or Inspect (Chrome).Once you click on the relevant Inspect menu item, the tools for web developers should appear within your browser. We want to look for the class and tags associated with the artists’ names in this list. We’ll see first that the table of names is within <div> tags where class=\"BodyText\". This is important to note so that we only search for text within this section of the web page. We also notice that the name Zabaglia, Niccola is in a link tag, since the name references a web page that describes the artist. So we will want to reference the <a> tag for links. Each artist’s name is a reference to a link. To do this, we’ll use Beautiful Soup’s find() and find_all() methods in order to pull the text of the artists’ names from the BodyText <div>. Next, at the bottom of our program file, we will want to create a for loop in order to iterate over all the artist names that we just put into the artist_name_list_items variable. We’ll print these names out with the prettify() method in order to turn the Beautiful Soup parse tree into a nicely formatted Unicode string.Let’s run the program as we have it so far:Once we do so, we’ll receive the following output:What we see in the output at this point is the full text and tags related to all of the artists’ names within the <a> tags found in the <div class=\"BodyText\"> tag on the first page, as well as some additional link text at the bottom. Since we don’t want this extra information, let’s work on removing this in the next section.So far, we have been able to collect all the link text data within one <div> section of our web page. However, we don’t want to have the bottom links that don’t reference artists’ names, so let’s work to remove that part. In order to remove the bottom links of the page, let’s again right-click and Inspect the DOM. We’ll see that the links on the bottom of the <div class=\"BodyText\"> section are contained in an HTML table: <table class=\"AlphaNav\">:We can therefore use Beautiful Soup to find the AlphaNav class and use the decompose() method to remove a tag from the parse tree and then destroy it along with its contents. We’ll use the variable last_links to reference these bottom links and add them to the program file:Now, when we run the program with the python nga_z_artist.py command, we’ll receive the following output:At this point, we see that the output no longer includes the links at the bottom of the web page, and now only displays the links associated with artists’ names. So far we have targeted the links with the artists’ names specifically, but we have the extra tag data that we don’t really want. Let’s remove that in the next section.In order to access only the actual artists’ names, we’ll want to target the contents of the <a> tags rather than print out the entire link tag.We can do this with Beautiful Soup’s .contents, which will return the tag’s children as a Python list data type.Let’s revise the for loop so that instead of printing the entire link and its tag, we’ll print the list of children (i.e. the artists’ full names):Note that we are iterating over the list above by calling on the index number of each item.We can run the program with the python command to view the following output:We have received back a list of all the artists’ names available on the first page of the letter Z. However, what if we want to also capture the URLs associated with those artists? We can extract URLs found within a page’s <a> tags by using Beautiful Soup’s get(\\'href\\') method. From the output of the links above, we know that the entire URL is not being captured, so we will concatenate the link string with the front of the URL string (in this case https://www.nga.gov).These lines we’ll also add to the for loop:When we run the program above, we’ll receive both the artists’ names and the URLs to the links that tell us more about the artists:Although we are now getting information from the website, it is currently just printing to our terminal window. Let’s instead capture this data so that we can use it elsewhere by writing it to a file.Collecting data that only lives in a terminal window is not very useful. Comma-separated values (CSV) files allows us to store tabular data in plain text, and is a common format for spreadsheets and databases. Before beginning with this section, you should familiarize yourself with how to handle plain text files in Python.First, we need to import Python’s built-in csv module along with the other modules at the top of the Python programming file:Next, we’ll create and open a file called z-artist-names.csv for us to write to (we’ll use the variable f for file here) by using the \\'w\\' mode. We’ll also write the top row headings: Name and Link which we’ll pass to the writerow() method as a list:Finally, within our for loop, we’ll write each row with the artists’ names and their associated links:You can see the lines for each of these tasks in the file below:When you run the program now with the python command, no output will be returned to your terminal window. Instead, a file will be created in the directory you are working in called z-artist-names.csv. Depending on what you use to open it, it may look something like this:Or, it may look more like a spreadsheet:In either case, you can now use this file to work with the data in more meaningful ways since the information you have collected is now stored in your computer’s memory.We have created a program that will pull data from the first page of the list of artists whose last names start with the letter Z. However, there are 5 pages in total of these artists available on the website.In order to collect all of these pages, we can perform more iterations with for loops. This will revise most of the code we have written so far, but will employ similar concepts.To start, we’ll want to initialize a list to hold the pages:We will populate this initialized list with the following for loop:Earlier in this tutorial, we noted that we should pay attention to the total number of pages there are that contain artists’ names starting with the letter Z (or whatever letter we’re using). Since there are 5 pages for the letter Z, we constructed the for loop above with a range of 1 to 6 so that it will iterate through each of the 5 pages. For this specific web site, the URLs begin with the string https://www.nga.gov/collection/anZ and then are followed with a number for the page (which will be the integer i from the for loop that we convert to a string) and end with .htm. We will concatenate these strings together and then append the result to the pages list.In addition to this loop, we’ll have a second loop that will go through each of the pages above. The code in this for loop will look similar to the code we have created so far, as it is doing the task we completed for the first page of the letter Z artists for each of the 5 pages total. Note that because we have put the original program into the second for loop, we now have the original loop as a nested for loop contained in it. The two for loops will look like this:In the code above, you should see that the first for loop is iterating over the pages and the second for loop is scraping data from each of those pages and then is adding the artists’ names and links line by line through each row of each page. These two for loops come below the import statements, the CSV file creation and writer (with the line for writing the headers of the file), and the initialization of the pages variable (assigned to a list).Within the greater context of the programming file, the complete code looks like this:Since this program is doing a bit of work, it will take a little while to create the CSV file. Once it is done, the output will be complete, showing the artists’ names and their associated links from Zabaglia, Niccola to Zykmund, Václav.When scraping web pages, it is important to remain considerate of the servers you are grabbing information from. Check to see if a site has terms of service or terms of use that pertains to web scraping. Also, check to see if a site has an API that allows you to grab data before scraping it yourself. Be sure to not continuously hit servers to gather data. Once you have collected what you need from a site, run scripts that will go over the data locally rather than burden someone else’s servers. Additionally, it is a good idea to scrape with a header that has your name and email so that a website can identify you and follow up if they have any questions. An example of a header you can use with the Python Requests library is as follows:Using headers with identifiable information ensures that the people who go over a server’s logs can reach out to you.This tutorial went through using Python and Beautiful Soup to scrape data from a website. We stored the text that we gathered within a CSV file.You can continue working on this project by collecting more data and making your CSV file more robust. For example, you may want to include the nationalities and years of each artist. You can also use what you have learned to scrape data from other websites. To continue learning about pulling information from the web, read our tutorial “How To Crawl A Web Page with Scrapy and Python 3.”Simple setup. Full root access. Straightforward pricing.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.replace('[]','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Get the latest tutorials on SysAdmin and open source topics  Many data analysis, big data, and machine learning projects require scraping websites to gather the data that you’ll be working with The Python programming language is widely used in the data science community, and therefore has an ecosystem of modules and tools that you can use in your own projects In this tutorial we will be focusing on the Beautiful Soup module Beautiful Soup, an allusion to the Mock Turtle’s song found in Chapter 10 of Lewis Carroll’s Alice’s Adventures in Wonderland, is a Python library that allows for quick turnaround on web scraping projects Currently available as Beautiful Soup 4 and compatible with both Python 2.7 and Python 3, Beautiful Soup creates a parse tree from parsed HTML and XML documents (including documents with non-closed tags or tag soup and other malformed markup) In this tutorial, we will collect and parse a web page in order to grab textual data and write the information we have gathered to a CSV file Before working on this tutorial, you should have a local or server-based Python programming environment set up on your machine.You should have the Requests and Beautiful Soup modules installed, which you can achieve by following our tutorial “How To Work with Web Data Using Requests and Beautiful Soup with Python 3.” It would also be useful to have a working familiarity with these modules Additionally, since we will be working with data scraped from the web, you should be comfortable with HTML structure and tagging In this tutorial, we’ll be working with data from the official website of the National Gallery of Art in the United States The National Gallery is an art museum located on the National Mall in Washington, D.C It holds over 120,000 pieces dated from the Renaissance to the present day done by more than 13,000 artists.We would like to search the Index of Artists, which is available at https://www.nga.gov/collection/an.shtm Since we’ll be doing this project in order to learn about web scraping with Beautiful Soup, we don’t need to pull too much data from the site, so let’s limit the scope of the artist data we are looking to scrape Let’s therefore choose one letter — in our example we’ll choose the letter Z — and we’ll see a page that looks like this:In the page above, we see that the first artist listed at the time of writing is Zabaglia, Niccola, which is a good thing to note for when we start pulling data We’ll start by working with this first page, with the following URL for the letter Z:It is important to note for later how many pages total there are for the letter you are choosing to list, which you can discover by clicking through to the last page of artists In this case, there are 5 pages total, and the last artist listed at the time of writing is Zykmund, Václav The last page of Z artists has the following URL:To begin to familiarize yourself with the DOM of this web page, you can open your browser’s Developer Tools.To begin our coding project, let’s activate our Python 3 programming environment Make sure you’re in the directory where your environment is located, and run the following command:With our programming environment activated, we’ll create a new file, with nano for instance You can name your file whatever you would like, we’ll call it nga_z_artists.py in this tutorial.Within this file, we can begin to import the libraries we’ll be using — Requests and Beautiful Soup.The Requests library allows you to make use of HTTP within your Python programs in a human readable way, and the Beautiful Soup module is designed to get web scraping done quickly.We will import both Requests and Beautiful Soup with the import statement For Beautiful Soup, we’ll be importing it from bs4, the package in which Beautiful Soup 4 is found With both the Requests and Beautiful Soup modules imported, we can move on to working to first collect a page and then parse it The next step we will need to do is collect the URL of the first web page with Requests We’ll assign the URL for the first page to the variable page by using the method requests.get().Next, we’ll create a BeautifulSoup object, or a parse tree This object takes as its arguments the page.text document from Requests (the content of the server’s response) and then parses it from Python’s built-in html.parser.Now with our page collected, parsed, and set up as a BeautifulSoup object, we can move on to collecting the data that we would like.For this project, we’ll collect artists’ names and the relevant links available on the website You may want to collect different data, such as the artists’ nationality and dates Whatever data you would like to collect, you need to find out how it is described by the DOM of the web page To do this, in your web browser, right-click — or CTRL + click on macOS — on the first artist’s name, Zabaglia, Niccola Within the context menu that pops up, you should see a menu item similar to Inspect Element (Firefox) or Inspect (Chrome).Once you click on the relevant Inspect menu item, the tools for web developers should appear within your browser We want to look for the class and tags associated with the artists’ names in this list We’ll see first that the table of names is within <div> tags where class=\"BodyText\" This is important to note so that we only search for text within this section of the web page We also notice that the name Zabaglia, Niccola is in a link tag, since the name references a web page that describes the artist So we will want to reference the <a> tag for links Each artist’s name is a reference to a link To do this, we’ll use Beautiful Soup’s find() and find_all() methods in order to pull the text of the artists’ names from the BodyText <div> Next, at the bottom of our program file, we will want to create a for loop in order to iterate over all the artist names that we just put into the artist_name_list_items variable We’ll print these names out with the prettify() method in order to turn the Beautiful Soup parse tree into a nicely formatted Unicode string.Let’s run the program as we have it so far:Once we do so, we’ll receive the following output:What we see in the output at this point is the full text and tags related to all of the artists’ names within the <a> tags found in the <div class=\"BodyText\"> tag on the first page, as well as some additional link text at the bottom Since we don’t want this extra information, let’s work on removing this in the next section.So far, we have been able to collect all the link text data within one <div> section of our web page However, we don’t want to have the bottom links that don’t reference artists’ names, so let’s work to remove that part In order to remove the bottom links of the page, let’s again right-click and Inspect the DOM We’ll see that the links on the bottom of the <div class=\"BodyText\"> section are contained in an HTML table: <table class=\"AlphaNav\">:We can therefore use Beautiful Soup to find the AlphaNav class and use the decompose() method to remove a tag from the parse tree and then destroy it along with its contents We’ll use the variable last_links to reference these bottom links and add them to the program file:Now, when we run the program with the python nga_z_artist.py command, we’ll receive the following output:At this point, we see that the output no longer includes the links at the bottom of the web page, and now only displays the links associated with artists’ names So far we have targeted the links with the artists’ names specifically, but we have the extra tag data that we don’t really want Let’s remove that in the next section.In order to access only the actual artists’ names, we’ll want to target the contents of the <a> tags rather than print out the entire link tag.We can do this with Beautiful Soup’s .contents, which will return the tag’s children as a Python list data type.Let’s revise the for loop so that instead of printing the entire link and its tag, we’ll print the list of children (i.e the artists’ full names):Note that we are iterating over the list above by calling on the index number of each item.We can run the program with the python command to view the following output:We have received back a list of all the artists’ names available on the first page of the letter Z However, what if we want to also capture the URLs associated with those artists? We can extract URLs found within a page’s <a> tags by using Beautiful Soup’s get(\\'href\\') method From the output of the links above, we know that the entire URL is not being captured, so we will concatenate the link string with the front of the URL string (in this case https://www.nga.gov).These lines we’ll also add to the for loop:When we run the program above, we’ll receive both the artists’ names and the URLs to the links that tell us more about the artists:Although we are now getting information from the website, it is currently just printing to our terminal window Let’s instead capture this data so that we can use it elsewhere by writing it to a file.Collecting data that only lives in a terminal window is not very useful Comma-separated values (CSV) files allows us to store tabular data in plain text, and is a common format for spreadsheets and databases Before beginning with this section, you should familiarize yourself with how to handle plain text files in Python.First, we need to import Python’s built-in csv module along with the other modules at the top of the Python programming file:Next, we’ll create and open a file called z-artist-names.csv for us to write to (we’ll use the variable f for file here) by using the \\'w\\' mode We’ll also write the top row headings: Name and Link which we’ll pass to the writerow() method as a list:Finally, within our for loop, we’ll write each row with the artists’ names and their associated links:You can see the lines for each of these tasks in the file below:When you run the program now with the python command, no output will be returned to your terminal window Instead, a file will be created in the directory you are working in called z-artist-names.csv Depending on what you use to open it, it may look something like this:Or, it may look more like a spreadsheet:In either case, you can now use this file to work with the data in more meaningful ways since the information you have collected is now stored in your computer’s memory.We have created a program that will pull data from the first page of the list of artists whose last names start with the letter Z However, there are 5 pages in total of these artists available on the website.In order to collect all of these pages, we can perform more iterations with for loops This will revise most of the code we have written so far, but will employ similar concepts.To start, we’ll want to initialize a list to hold the pages:We will populate this initialized list with the following for loop:Earlier in this tutorial, we noted that we should pay attention to the total number of pages there are that contain artists’ names starting with the letter Z (or whatever letter we’re using) Since there are 5 pages for the letter Z, we constructed the for loop above with a range of 1 to 6 so that it will iterate through each of the 5 pages For this specific web site, the URLs begin with the string https://www.nga.gov/collection/anZ and then are followed with a number for the page (which will be the integer i from the for loop that we convert to a string) and end with .htm We will concatenate these strings together and then append the result to the pages list.In addition to this loop, we’ll have a second loop that will go through each of the pages above The code in this for loop will look similar to the code we have created so far, as it is doing the task we completed for the first page of the letter Z artists for each of the 5 pages total Note that because we have put the original program into the second for loop, we now have the original loop as a nested for loop contained in it The two for loops will look like this:In the code above, you should see that the first for loop is iterating over the pages and the second for loop is scraping data from each of those pages and then is adding the artists’ names and links line by line through each row of each page These two for loops come below the import statements, the CSV file creation and writer (with the line for writing the headers of the file), and the initialization of the pages variable (assigned to a list).Within the greater context of the programming file, the complete code looks like this:Since this program is doing a bit of work, it will take a little while to create the CSV file Once it is done, the output will be complete, showing the artists’ names and their associated links from Zabaglia, Niccola to Zykmund, Václav.When scraping web pages, it is important to remain considerate of the servers you are grabbing information from Check to see if a site has terms of service or terms of use that pertains to web scraping Also, check to see if a site has an API that allows you to grab data before scraping it yourself Be sure to not continuously hit servers to gather data Once you have collected what you need from a site, run scripts that will go over the data locally rather than burden someone else’s servers Additionally, it is a good idea to scrape with a header that has your name and email so that a website can identify you and follow up if they have any questions An example of a header you can use with the Python Requests library is as follows:Using headers with identifiable information ensures that the people who go over a server’s logs can reach out to you.This tutorial went through using Python and Beautiful Soup to scrape data from a website We stored the text that we gathered within a CSV file.You can continue working on this project by collecting more data and making your CSV file more robust For example, you may want to include the nationalities and years of each artist You can also use what you have learned to scrape data from other websites To continue learning about pulling information from the web, read our tutorial “How To Crawl A Web Page with Scrapy and Python 3.”Simple setup Full root access Straightforward pricing.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.replace('. ',\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Get the latest tutorials on SysAdmin and open source topics.  Many data analysis, big data, and machine learning projects require scraping websites to gather the data that you’ll be working with. The Python programming language is widely used in the data science community, and therefore has an ecosystem of modules and tools that you can use in your own projects. In this tutorial we will be focusing on the Beautiful Soup module. Beautiful Soup, an allusion to the Mock Turtle’s song found in Chapter 10 of Lewis Carroll’s Alice’s Adventures in Wonderland, is a Python library that allows for quick turnaround on web scraping projects. Currently available as Beautiful Soup 4 and compatible with both Python 2.7 and Python 3, Beautiful Soup creates a parse tree from parsed HTML and XML documents (including documents with non-closed tags or tag soup and other malformed markup). In this tutorial, we will collect and parse a web page in order to grab textual data and write the information we have gathered to a CSV file. Before working on this tutorial, you should have a local or server-based Python programming environment set up on your machine.You should have the Requests and Beautiful Soup modules installed, which you can achieve by following our tutorial “How To Work with Web Data Using Requests and Beautiful Soup with Python 3.” It would also be useful to have a working familiarity with these modules. Additionally, since we will be working with data scraped from the web, you should be comfortable with HTML structure and tagging. In this tutorial, we’ll be working with data from the official website of the National Gallery of Art in the United States. The National Gallery is an art museum located on the National Mall in Washington, D.C. It holds over 120,000 pieces dated from the Renaissance to the present day done by more than 13,000 artists.We would like to search the Index of Artists, which is available at https://www.nga.gov/collection/an.shtm. Since we’ll be doing this project in order to learn about web scraping with Beautiful Soup, we don’t need to pull too much data from the site, so let’s limit the scope of the artist data we are looking to scrape. Let’s therefore choose one letter — in our example we’ll choose the letter Z — and we’ll see a page that looks like this:In the page above, we see that the first artist listed at the time of writing is Zabaglia, Niccola, which is a good thing to note for when we start pulling data. We’ll start by working with this first page, with the following URL for the letter Z:It is important to note for later how many pages total there are for the letter you are choosing to list, which you can discover by clicking through to the last page of artists. In this case, there are 5 pages total, and the last artist listed at the time of writing is Zykmund, Václav. The last page of Z artists has the following URL:To begin to familiarize yourself with the DOM of this web page, you can open your browser’s Developer Tools.To begin our coding project, let’s activate our Python 3 programming environment. Make sure you’re in the directory where your environment is located, and run the following command:With our programming environment activated, we’ll create a new file, with nano for instance. You can name your file whatever you would like, we’ll call it nga_z_artists.py in this tutorial.Within this file, we can begin to import the libraries we’ll be using — Requests and Beautiful Soup.The Requests library allows you to make use of HTTP within your Python programs in a human readable way, and the Beautiful Soup module is designed to get web scraping done quickly.We will import both Requests and Beautiful Soup with the import statement. For Beautiful Soup, we’ll be importing it from bs4, the package in which Beautiful Soup 4 is found. With both the Requests and Beautiful Soup modules imported, we can move on to working to first collect a page and then parse it. The next step we will need to do is collect the URL of the first web page with Requests. We’ll assign the URL for the first page to the variable page by using the method requests.get().Next, we’ll create a BeautifulSoup object, or a parse tree. This object takes as its arguments the page.text document from Requests (the content of the server’s response) and then parses it from Python’s built-in html.parser.Now with our page collected, parsed, and set up as a BeautifulSoup object, we can move on to collecting the data that we would like.For this project, we’ll collect artists’ names and the relevant links available on the website. You may want to collect different data, such as the artists’ nationality and dates. Whatever data you would like to collect, you need to find out how it is described by the DOM of the web page. To do this, in your web browser, right-click — or CTRL + click on macOS — on the first artist’s name, Zabaglia, Niccola. Within the context menu that pops up, you should see a menu item similar to Inspect Element (Firefox) or Inspect (Chrome).Once you click on the relevant Inspect menu item, the tools for web developers should appear within your browser. We want to look for the class and tags associated with the artists’ names in this list. We’ll see first that the table of names is within <div> tags where class=\"BodyText\". This is important to note so that we only search for text within this section of the web page. We also notice that the name Zabaglia, Niccola is in a link tag, since the name references a web page that describes the artist. So we will want to reference the <a> tag for links. Each artist’s name is a reference to a link. To do this, we’ll use Beautiful Soup’s find() and find_all() methods in order to pull the text of the artists’ names from the BodyText <div>. Next, at the bottom of our program file, we will want to create a for loop in order to iterate over all the artist names that we just put into the artist_name_list_items variable. We’ll print these names out with the prettify() method in order to turn the Beautiful Soup parse tree into a nicely formatted Unicode string.Let’s run the program as we have it so far:Once we do so, we’ll receive the following output:What we see in the output at this point is the full text and tags related to all of the artists’ names within the <a> tags found in the <div class=\"BodyText\"> tag on the first page, as well as some additional link text at the bottom. Since we don’t want this extra information, let’s work on removing this in the next section.So far, we have been able to collect all the link text data within one <div> section of our web page. However, we don’t want to have the bottom links that don’t reference artists’ names, so let’s work to remove that part. In order to remove the bottom links of the page, let’s again right-click and Inspect the DOM. We’ll see that the links on the bottom of the <div class=\"BodyText\"> section are contained in an HTML table: <table class=\"AlphaNav\">:We can therefore use Beautiful Soup to find the AlphaNav class and use the decompose() method to remove a tag from the parse tree and then destroy it along with its contents. We’ll use the variable last_links to reference these bottom links and add them to the program file:Now, when we run the program with the python nga_z_artist.py command, we’ll receive the following output:At this point, we see that the output no longer includes the links at the bottom of the web page, and now only displays the links associated with artists’ names. So far we have targeted the links with the artists’ names specifically, but we have the extra tag data that we don’t really want. Let’s remove that in the next section.In order to access only the actual artists’ names, we’ll want to target the contents of the <a> tags rather than print out the entire link tag.We can do this with Beautiful Soup’s .contents, which will return the tag’s children as a Python list data type.Let’s revise the for loop so that instead of printing the entire link and its tag, we’ll print the list of children (i.e. the artists’ full names):Note that we are iterating over the list above by calling on the index number of each item.We can run the program with the python command to view the following output:We have received back a list of all the artists’ names available on the first page of the letter Z. However, what if we want to also capture the URLs associated with those artists? We can extract URLs found within a page’s <a> tags by using Beautiful Soup’s get(\\'href\\') method. From the output of the links above, we know that the entire URL is not being captured, so we will concatenate the link string with the front of the URL string (in this case https://www.nga.gov).These lines we’ll also add to the for loop:When we run the program above, we’ll receive both the artists’ names and the URLs to the links that tell us more about the artists:Although we are now getting information from the website, it is currently just printing to our terminal window. Let’s instead capture this data so that we can use it elsewhere by writing it to a file.Collecting data that only lives in a terminal window is not very useful. Comma-separated values (CSV) files allows us to store tabular data in plain text, and is a common format for spreadsheets and databases. Before beginning with this section, you should familiarize yourself with how to handle plain text files in Python.First, we need to import Python’s built-in csv module along with the other modules at the top of the Python programming file:Next, we’ll create and open a file called z-artist-names.csv for us to write to (we’ll use the variable f for file here) by using the \\'w\\' mode. We’ll also write the top row headings: Name and Link which we’ll pass to the writerow() method as a list:Finally, within our for loop, we’ll write each row with the artists’ names and their associated links:You can see the lines for each of these tasks in the file below:When you run the program now with the python command, no output will be returned to your terminal window. Instead, a file will be created in the directory you are working in called z-artist-names.csv. Depending on what you use to open it, it may look something like this:Or, it may look more like a spreadsheet:In either case, you can now use this file to work with the data in more meaningful ways since the information you have collected is now stored in your computer’s memory.We have created a program that will pull data from the first page of the list of artists whose last names start with the letter Z. However, there are 5 pages in total of these artists available on the website.In order to collect all of these pages, we can perform more iterations with for loops. This will revise most of the code we have written so far, but will employ similar concepts.To start, we’ll want to initialize a list to hold the pages:We will populate this initialized list with the following for loop:Earlier in this tutorial, we noted that we should pay attention to the total number of pages there are that contain artists’ names starting with the letter Z (or whatever letter we’re using). Since there are 5 pages for the letter Z, we constructed the for loop above with a range of 1 to 6 so that it will iterate through each of the 5 pages. For this specific web site, the URLs begin with the string https://www.nga.gov/collection/anZ and then are followed with a number for the page (which will be the integer i from the for loop that we convert to a string) and end with .htm. We will concatenate these strings together and then append the result to the pages list.In addition to this loop, we’ll have a second loop that will go through each of the pages above. The code in this for loop will look similar to the code we have created so far, as it is doing the task we completed for the first page of the letter Z artists for each of the 5 pages total. Note that because we have put the original program into the second for loop, we now have the original loop as a nested for loop contained in it. The two for loops will look like this:In the code above, you should see that the first for loop is iterating over the pages and the second for loop is scraping data from each of those pages and then is adding the artists’ names and links line by line through each row of each page. These two for loops come below the import statements, the CSV file creation and writer (with the line for writing the headers of the file), and the initialization of the pages variable (assigned to a list).Within the greater context of the programming file, the complete code looks like this:Since this program is doing a bit of work, it will take a little while to create the CSV file. Once it is done, the output will be complete, showing the artists’ names and their associated links from Zabaglia, Niccola to Zykmund, Václav.When scraping web pages, it is important to remain considerate of the servers you are grabbing information from. Check to see if a site has terms of service or terms of use that pertains to web scraping. Also, check to see if a site has an API that allows you to grab data before scraping it yourself. Be sure to not continuously hit servers to gather data. Once you have collected what you need from a site, run scripts that will go over the data locally rather than burden someone else’s servers. Additionally, it is a good idea to scrape with a header that has your name and email so that a website can identify you and follow up if they have any questions. An example of a header you can use with the Python Requests library is as follows:Using headers with identifiable information ensures that the people who go over a server’s logs can reach out to you.This tutorial went through using Python and Beautiful Soup to scrape data from a website. We stored the text that we gathered within a CSV file.You can continue working on this project by collecting more data and making your CSV file more robust. For example, you may want to include the nationalities and years of each artist. You can also use what you have learned to scrape data from other websites. To continue learning about pulling information from the web, read our tutorial “How To Crawl A Web Page with Scrapy and Python 3.”Simple setup. Full root access. Straightforward pricing.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Get the latest tutorials on SysAdmin and open source topics.  Many data analysis, big data, and machine learning projects require scraping websites to gather the data that you’ll be working with. The Python programming language is widely used in the data science community, and therefore has an ecosystem of modules and tools that you can use in your own projects. In this tutorial we will be focusing on the Beautiful Soup module. Beautiful Soup, an allusion to the Mock Turtle’s song found in Chapter 10 of Lewis Carroll’s Alice’s Adventures in Wonderland, is a Python library that allows for quick turnaround on web scraping projects. Currently available as Beautiful Soup 4 and compatible with both Python 2.7 and Python 3, Beautiful Soup creates a parse tree from parsed HTML and XML documents (including documents with non-closed tags or tag soup and other malformed markup). In this tutorial, we will collect and parse a web page in order to grab textual data and write the information we have gathered to a CSV file. Before working on this tutorial, you should have a local or server-based Python programming environment set up on your machine.You should have the Requests and Beautiful Soup modules installed, which you can achieve by following our tutorial “How To Work with Web Data Using Requests and Beautiful Soup with Python 3.” It would also be useful to have a working familiarity with these modules. Additionally, since we will be working with data scraped from the web, you should be comfortable with HTML structure and tagging. In this tutorial, we’ll be working with data from the official website of the National Gallery of Art in the United States. The National Gallery is an art museum located on the National Mall in Washington, D.C. It holds over 120,000 pieces dated from the Renaissance to the present day done by more than 13,000 artists.We would like to search the Index of Artists, which is available at https://www.nga.gov/collection/an.shtm. Since we’ll be doing this project in order to learn about web scraping with Beautiful Soup, we don’t need to pull too much data from the site, so let’s limit the scope of the artist data we are looking to scrape. Let’s therefore choose one letter — in our example we’ll choose the letter Z — and we’ll see a page that looks like this:In the page above, we see that the first artist listed at the time of writing is Zabaglia, Niccola, which is a good thing to note for when we start pulling data. We’ll start by working with this first page, with the following URL for the letter Z:It is important to note for later how many pages total there are for the letter you are choosing to list, which you can discover by clicking through to the last page of artists. In this case, there are 5 pages total, and the last artist listed at the time of writing is Zykmund, Václav. The last page of Z artists has the following URL:To begin to familiarize yourself with the DOM of this web page, you can open your browser’s Developer Tools.To begin our coding project, let’s activate our Python 3 programming environment. Make sure you’re in the directory where your environment is located, and run the following command:With our programming environment activated, we’ll create a new file, with nano for instance. You can name your file whatever you would like, we’ll call it nga_z_artists.py in this tutorial.Within this file, we can begin to import the libraries we’ll be using — Requests and Beautiful Soup.The Requests library allows you to make use of HTTP within your Python programs in a human readable way, and the Beautiful Soup module is designed to get web scraping done quickly.We will import both Requests and Beautiful Soup with the import statement. For Beautiful Soup, we’ll be importing it from bs4, the package in which Beautiful Soup 4 is found. With both the Requests and Beautiful Soup modules imported, we can move on to working to first collect a page and then parse it. The next step we will need to do is collect the URL of the first web page with Requests. We’ll assign the URL for the first page to the variable page by using the method requests.get().Next, we’ll create a BeautifulSoup object, or a parse tree. This object takes as its arguments the page.text document from Requests (the content of the server’s response) and then parses it from Python’s built-in html.parser.Now with our page collected, parsed, and set up as a BeautifulSoup object, we can move on to collecting the data that we would like.For this project, we’ll collect artists’ names and the relevant links available on the website. You may want to collect different data, such as the artists’ nationality and dates. Whatever data you would like to collect, you need to find out how it is described by the DOM of the web page. To do this, in your web browser, right-click — or CTRL + click on macOS — on the first artist’s name, Zabaglia, Niccola. Within the context menu that pops up, you should see a menu item similar to Inspect Element (Firefox) or Inspect (Chrome).Once you click on the relevant Inspect menu item, the tools for web developers should appear within your browser. We want to look for the class and tags associated with the artists’ names in this list. We’ll see first that the table of names is within <div> tags where class=\"BodyText\". This is important to note so that we only search for text within this section of the web page. We also notice that the name Zabaglia, Niccola is in a link tag, since the name references a web page that describes the artist. So we will want to reference the <a> tag for links. Each artist’s name is a reference to a link. To do this, we’ll use Beautiful Soup’s find() and find_all() methods in order to pull the text of the artists’ names from the BodyText <div>. Next, at the bottom of our program file, we will want to create a for loop in order to iterate over all the artist names that we just put into the artist_name_list_items variable. We’ll print these names out with the prettify() method in order to turn the Beautiful Soup parse tree into a nicely formatted Unicode string.Let’s run the program as we have it so far:Once we do so, we’ll receive the following output:What we see in the output at this point is the full text and tags related to all of the artists’ names within the <a> tags found in the <div class=\"BodyText\"> tag on the first page, as well as some additional link text at the bottom. Since we don’t want this extra information, let’s work on removing this in the next section.So far, we have been able to collect all the link text data within one <div> section of our web page. However, we don’t want to have the bottom links that don’t reference artists’ names, so let’s work to remove that part. In order to remove the bottom links of the page, let’s again right-click and Inspect the DOM. We’ll see that the links on the bottom of the <div class=\"BodyText\"> section are contained in an HTML table: <table class=\"AlphaNav\">:We can therefore use Beautiful Soup to find the AlphaNav class and use the decompose() method to remove a tag from the parse tree and then destroy it along with its contents. We’ll use the variable last_links to reference these bottom links and add them to the program file:Now, when we run the program with the python nga_z_artist.py command, we’ll receive the following output:At this point, we see that the output no longer includes the links at the bottom of the web page, and now only displays the links associated with artists’ names. So far we have targeted the links with the artists’ names specifically, but we have the extra tag data that we don’t really want. Let’s remove that in the next section.In order to access only the actual artists’ names, we’ll want to target the contents of the <a> tags rather than print out the entire link tag.We can do this with Beautiful Soup’s .contents, which will return the tag’s children as a Python list data type.Let’s revise the for loop so that instead of printing the entire link and its tag, we’ll print the list of children (i.e. the artists’ full names):Note that we are iterating over the list above by calling on the index number of each item.We can run the program with the python command to view the following output:We have received back a list of all the artists’ names available on the first page of the letter Z. However, what if we want to also capture the URLs associated with those artists? We can extract URLs found within a page’s <a> tags by using Beautiful Soup’s get(\\'href\\') method. From the output of the links above, we know that the entire URL is not being captured, so we will concatenate the link string with the front of the URL string (in this case https://www.nga.gov).These lines we’ll also add to the for loop:When we run the program above, we’ll receive both the artists’ names and the URLs to the links that tell us more about the artists:Although we are now getting information from the website, it is currently just printing to our terminal window. Let’s instead capture this data so that we can use it elsewhere by writing it to a file.Collecting data that only lives in a terminal window is not very useful. Comma-separated values (CSV) files allows us to store tabular data in plain text, and is a common format for spreadsheets and databases. Before beginning with this section, you should familiarize yourself with how to handle plain text files in Python.First, we need to import Python’s built-in csv module along with the other modules at the top of the Python programming file:Next, we’ll create and open a file called z-artist-names.csv for us to write to (we’ll use the variable f for file here) by using the \\'w\\' mode. We’ll also write the top row headings: Name and Link which we’ll pass to the writerow() method as a list:Finally, within our for loop, we’ll write each row with the artists’ names and their associated links:You can see the lines for each of these tasks in the file below:When you run the program now with the python command, no output will be returned to your terminal window. Instead, a file will be created in the directory you are working in called z-artist-names.csv. Depending on what you use to open it, it may look something like this:Or, it may look more like a spreadsheet:In either case, you can now use this file to work with the data in more meaningful ways since the information you have collected is now stored in your computer’s memory.We have created a program that will pull data from the first page of the list of artists whose last names start with the letter Z. However, there are 5 pages in total of these artists available on the website.In order to collect all of these pages, we can perform more iterations with for loops. This will revise most of the code we have written so far, but will employ similar concepts.To start, we’ll want to initialize a list to hold the pages:We will populate this initialized list with the following for loop:Earlier in this tutorial, we noted that we should pay attention to the total number of pages there are that contain artists’ names starting with the letter Z (or whatever letter we’re using). Since there are 5 pages for the letter Z, we constructed the for loop above with a range of 1 to 6 so that it will iterate through each of the 5 pages. For this specific web site, the URLs begin with the string https://www.nga.gov/collection/anZ and then are followed with a number for the page (which will be the integer i from the for loop that we convert to a string) and end with .htm. We will concatenate these strings together and then append the result to the pages list.In addition to this loop, we’ll have a second loop that will go through each of the pages above. The code in this for loop will look similar to the code we have created so far, as it is doing the task we completed for the first page of the letter Z artists for each of the 5 pages total. Note that because we have put the original program into the second for loop, we now have the original loop as a nested for loop contained in it. The two for loops will look like this:In the code above, you should see that the first for loop is iterating over the pages and the second for loop is scraping data from each of those pages and then is adding the artists’ names and links line by line through each row of each page. These two for loops come below the import statements, the CSV file creation and writer (with the line for writing the headers of the file), and the initialization of the pages variable (assigned to a list).Within the greater context of the programming file, the complete code looks like this:Since this program is doing a bit of work, it will take a little while to create the CSV file. Once it is done, the output will be complete, showing the artists’ names and their associated links from Zabaglia, Niccola to Zykmund, Václav.When scraping web pages, it is important to remain considerate of the servers you are grabbing information from. Check to see if a site has terms of service or terms of use that pertains to web scraping. Also, check to see if a site has an API that allows you to grab data before scraping it yourself. Be sure to not continuously hit servers to gather data. Once you have collected what you need from a site, run scripts that will go over the data locally rather than burden someone else’s servers. Additionally, it is a good idea to scrape with a header that has your name and email so that a website can identify you and follow up if they have any questions. An example of a header you can use with the Python Requests library is as follows:Using headers with identifiable information ensures that the people who go over a server’s logs can reach out to you.This tutorial went through using Python and Beautiful Soup to scrape data from a website. We stored the text that we gathered within a CSV file.You can continue working on this project by collecting more data and making your CSV file more robust. For example, you may want to include the nationalities and years of each artist. You can also use what you have learned to scrape data from other websites. To continue learning about pulling information from the web, read our tutorial “How To Crawl A Web Page with Scrapy and Python 3.”Simple setup. Full root access. Straightforward pricing.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e=data\n",
    "data=data.strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Get the latest tutorials on SysAdmin and open source topics.  Many data analysis, big data, and machine learning projects require scraping websites to gather the data that you’ll be working with. The Python programming language is widely used in the data science community, and therefore has an ecosystem of modules and tools that you can use in your own projects. In this tutorial we will be focusing on the Beautiful Soup module. Beautiful Soup, an allusion to the Mock Turtle’s song found in Chapter 10 of Lewis Carroll’s Alice’s Adventures in Wonderland, is a Python library that allows for quick turnaround on web scraping projects. Currently available as Beautiful Soup 4 and compatible with both Python 2.7 and Python 3, Beautiful Soup creates a parse tree from parsed HTML and XML documents (including documents with non-closed tags or tag soup and other malformed markup). In this tutorial, we will collect and parse a web page in order to grab textual data and write the information we have gathered to a CSV file. Before working on this tutorial, you should have a local or server-based Python programming environment set up on your machine.You should have the Requests and Beautiful Soup modules installed, which you can achieve by following our tutorial “How To Work with Web Data Using Requests and Beautiful Soup with Python 3.” It would also be useful to have a working familiarity with these modules. Additionally, since we will be working with data scraped from the web, you should be comfortable with HTML structure and tagging. In this tutorial, we’ll be working with data from the official website of the National Gallery of Art in the United States. The National Gallery is an art museum located on the National Mall in Washington, D.C. It holds over 120,000 pieces dated from the Renaissance to the present day done by more than 13,000 artists.We would like to search the Index of Artists, which is available at https://www.nga.gov/collection/an.shtm. Since we’ll be doing this project in order to learn about web scraping with Beautiful Soup, we don’t need to pull too much data from the site, so let’s limit the scope of the artist data we are looking to scrape. Let’s therefore choose one letter — in our example we’ll choose the letter Z — and we’ll see a page that looks like this:In the page above, we see that the first artist listed at the time of writing is Zabaglia, Niccola, which is a good thing to note for when we start pulling data. We’ll start by working with this first page, with the following URL for the letter Z:It is important to note for later how many pages total there are for the letter you are choosing to list, which you can discover by clicking through to the last page of artists. In this case, there are 5 pages total, and the last artist listed at the time of writing is Zykmund, Václav. The last page of Z artists has the following URL:To begin to familiarize yourself with the DOM of this web page, you can open your browser’s Developer Tools.To begin our coding project, let’s activate our Python 3 programming environment. Make sure you’re in the directory where your environment is located, and run the following command:With our programming environment activated, we’ll create a new file, with nano for instance. You can name your file whatever you would like, we’ll call it nga_z_artists.py in this tutorial.Within this file, we can begin to import the libraries we’ll be using — Requests and Beautiful Soup.The Requests library allows you to make use of HTTP within your Python programs in a human readable way, and the Beautiful Soup module is designed to get web scraping done quickly.We will import both Requests and Beautiful Soup with the import statement. For Beautiful Soup, we’ll be importing it from bs4, the package in which Beautiful Soup 4 is found. With both the Requests and Beautiful Soup modules imported, we can move on to working to first collect a page and then parse it. The next step we will need to do is collect the URL of the first web page with Requests. We’ll assign the URL for the first page to the variable page by using the method requests.get().Next, we’ll create a BeautifulSoup object, or a parse tree. This object takes as its arguments the page.text document from Requests (the content of the server’s response) and then parses it from Python’s built-in html.parser.Now with our page collected, parsed, and set up as a BeautifulSoup object, we can move on to collecting the data that we would like.For this project, we’ll collect artists’ names and the relevant links available on the website. You may want to collect different data, such as the artists’ nationality and dates. Whatever data you would like to collect, you need to find out how it is described by the DOM of the web page. To do this, in your web browser, right-click — or CTRL + click on macOS — on the first artist’s name, Zabaglia, Niccola. Within the context menu that pops up, you should see a menu item similar to Inspect Element (Firefox) or Inspect (Chrome).Once you click on the relevant Inspect menu item, the tools for web developers should appear within your browser. We want to look for the class and tags associated with the artists’ names in this list. We’ll see first that the table of names is within <div> tags where class=\"BodyText\". This is important to note so that we only search for text within this section of the web page. We also notice that the name Zabaglia, Niccola is in a link tag, since the name references a web page that describes the artist. So we will want to reference the <a> tag for links. Each artist’s name is a reference to a link. To do this, we’ll use Beautiful Soup’s find() and find_all() methods in order to pull the text of the artists’ names from the BodyText <div>. Next, at the bottom of our program file, we will want to create a for loop in order to iterate over all the artist names that we just put into the artist_name_list_items variable. We’ll print these names out with the prettify() method in order to turn the Beautiful Soup parse tree into a nicely formatted Unicode string.Let’s run the program as we have it so far:Once we do so, we’ll receive the following output:What we see in the output at this point is the full text and tags related to all of the artists’ names within the <a> tags found in the <div class=\"BodyText\"> tag on the first page, as well as some additional link text at the bottom. Since we don’t want this extra information, let’s work on removing this in the next section.So far, we have been able to collect all the link text data within one <div> section of our web page. However, we don’t want to have the bottom links that don’t reference artists’ names, so let’s work to remove that part. In order to remove the bottom links of the page, let’s again right-click and Inspect the DOM. We’ll see that the links on the bottom of the <div class=\"BodyText\"> section are contained in an HTML table: <table class=\"AlphaNav\">:We can therefore use Beautiful Soup to find the AlphaNav class and use the decompose() method to remove a tag from the parse tree and then destroy it along with its contents. We’ll use the variable last_links to reference these bottom links and add them to the program file:Now, when we run the program with the python nga_z_artist.py command, we’ll receive the following output:At this point, we see that the output no longer includes the links at the bottom of the web page, and now only displays the links associated with artists’ names. So far we have targeted the links with the artists’ names specifically, but we have the extra tag data that we don’t really want. Let’s remove that in the next section.In order to access only the actual artists’ names, we’ll want to target the contents of the <a> tags rather than print out the entire link tag.We can do this with Beautiful Soup’s .contents, which will return the tag’s children as a Python list data type.Let’s revise the for loop so that instead of printing the entire link and its tag, we’ll print the list of children (i.e. the artists’ full names):Note that we are iterating over the list above by calling on the index number of each item.We can run the program with the python command to view the following output:We have received back a list of all the artists’ names available on the first page of the letter Z. However, what if we want to also capture the URLs associated with those artists? We can extract URLs found within a page’s <a> tags by using Beautiful Soup’s get(\\'href\\') method. From the output of the links above, we know that the entire URL is not being captured, so we will concatenate the link string with the front of the URL string (in this case https://www.nga.gov).These lines we’ll also add to the for loop:When we run the program above, we’ll receive both the artists’ names and the URLs to the links that tell us more about the artists:Although we are now getting information from the website, it is currently just printing to our terminal window. Let’s instead capture this data so that we can use it elsewhere by writing it to a file.Collecting data that only lives in a terminal window is not very useful. Comma-separated values (CSV) files allows us to store tabular data in plain text, and is a common format for spreadsheets and databases. Before beginning with this section, you should familiarize yourself with how to handle plain text files in Python.First, we need to import Python’s built-in csv module along with the other modules at the top of the Python programming file:Next, we’ll create and open a file called z-artist-names.csv for us to write to (we’ll use the variable f for file here) by using the \\'w\\' mode. We’ll also write the top row headings: Name and Link which we’ll pass to the writerow() method as a list:Finally, within our for loop, we’ll write each row with the artists’ names and their associated links:You can see the lines for each of these tasks in the file below:When you run the program now with the python command, no output will be returned to your terminal window. Instead, a file will be created in the directory you are working in called z-artist-names.csv. Depending on what you use to open it, it may look something like this:Or, it may look more like a spreadsheet:In either case, you can now use this file to work with the data in more meaningful ways since the information you have collected is now stored in your computer’s memory.We have created a program that will pull data from the first page of the list of artists whose last names start with the letter Z. However, there are 5 pages in total of these artists available on the website.In order to collect all of these pages, we can perform more iterations with for loops. This will revise most of the code we have written so far, but will employ similar concepts.To start, we’ll want to initialize a list to hold the pages:We will populate this initialized list with the following for loop:Earlier in this tutorial, we noted that we should pay attention to the total number of pages there are that contain artists’ names starting with the letter Z (or whatever letter we’re using). Since there are 5 pages for the letter Z, we constructed the for loop above with a range of 1 to 6 so that it will iterate through each of the 5 pages. For this specific web site, the URLs begin with the string https://www.nga.gov/collection/anZ and then are followed with a number for the page (which will be the integer i from the for loop that we convert to a string) and end with .htm. We will concatenate these strings together and then append the result to the pages list.In addition to this loop, we’ll have a second loop that will go through each of the pages above. The code in this for loop will look similar to the code we have created so far, as it is doing the task we completed for the first page of the letter Z artists for each of the 5 pages total. Note that because we have put the original program into the second for loop, we now have the original loop as a nested for loop contained in it. The two for loops will look like this:In the code above, you should see that the first for loop is iterating over the pages and the second for loop is scraping data from each of those pages and then is adding the artists’ names and links line by line through each row of each page. These two for loops come below the import statements, the CSV file creation and writer (with the line for writing the headers of the file), and the initialization of the pages variable (assigned to a list).Within the greater context of the programming file, the complete code looks like this:Since this program is doing a bit of work, it will take a little while to create the CSV file. Once it is done, the output will be complete, showing the artists’ names and their associated links from Zabaglia, Niccola to Zykmund, Václav.When scraping web pages, it is important to remain considerate of the servers you are grabbing information from. Check to see if a site has terms of service or terms of use that pertains to web scraping. Also, check to see if a site has an API that allows you to grab data before scraping it yourself. Be sure to not continuously hit servers to gather data. Once you have collected what you need from a site, run scripts that will go over the data locally rather than burden someone else’s servers. Additionally, it is a good idea to scrape with a header that has your name and email so that a website can identify you and follow up if they have any questions. An example of a header you can use with the Python Requests library is as follows:Using headers with identifiable information ensures that the people who go over a server’s logs can reach out to you.This tutorial went through using Python and Beautiful Soup to scrape data from a website. We stored the text that we gathered within a CSV file.You can continue working on this project by collecting more data and making your CSV file more robust. For example, you may want to include the nationalities and years of each artist. You can also use what you have learned to scrape data from other websites. To continue learning about pulling information from the web, read our tutorial “How To Crawl A Web Page with Scrapy and Python 3.”Simple setup. Full root access. Straightforward pricing.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data.split('. ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    Get the latest tutorials on SysAdmin and open source topics',\n",
       " ' Many data analysis, big data, and machine learning projects require scraping websites to gather the data that you’ll be working with',\n",
       " 'The Python programming language is widely used in the data science community, and therefore has an ecosystem of modules and tools that you can use in your own projects',\n",
       " 'In this tutorial we will be focusing on the Beautiful Soup module',\n",
       " 'Beautiful Soup, an allusion to the Mock Turtle’s song found in Chapter 10 of Lewis Carroll’s Alice’s Adventures in Wonderland, is a Python library that allows for quick turnaround on web scraping projects',\n",
       " 'Currently available as Beautiful Soup 4 and compatible with both Python 2.7 and Python 3, Beautiful Soup creates a parse tree from parsed HTML and XML documents (including documents with non-closed tags or tag soup and other malformed markup)',\n",
       " 'In this tutorial, we will collect and parse a web page in order to grab textual data and write the information we have gathered to a CSV file',\n",
       " 'Before working on this tutorial, you should have a local or server-based Python programming environment set up on your machine.You should have the Requests and Beautiful Soup modules installed, which you can achieve by following our tutorial “How To Work with Web Data Using Requests and Beautiful Soup with Python 3.” It would also be useful to have a working familiarity with these modules',\n",
       " 'Additionally, since we will be working with data scraped from the web, you should be comfortable with HTML structure and tagging',\n",
       " 'In this tutorial, we’ll be working with data from the official website of the National Gallery of Art in the United States',\n",
       " 'The National Gallery is an art museum located on the National Mall in Washington, D.C',\n",
       " 'It holds over 120,000 pieces dated from the Renaissance to the present day done by more than 13,000 artists.We would like to search the Index of Artists, which is available at https://www.nga.gov/collection/an.shtm',\n",
       " 'Since we’ll be doing this project in order to learn about web scraping with Beautiful Soup, we don’t need to pull too much data from the site, so let’s limit the scope of the artist data we are looking to scrape',\n",
       " 'Let’s therefore choose one letter — in our example we’ll choose the letter Z — and we’ll see a page that looks like this:In the page above, we see that the first artist listed at the time of writing is Zabaglia, Niccola, which is a good thing to note for when we start pulling data',\n",
       " 'We’ll start by working with this first page, with the following URL for the letter Z:It is important to note for later how many pages total there are for the letter you are choosing to list, which you can discover by clicking through to the last page of artists',\n",
       " 'In this case, there are 5 pages total, and the last artist listed at the time of writing is Zykmund, Václav',\n",
       " 'The last page of Z artists has the following URL:To begin to familiarize yourself with the DOM of this web page, you can open your browser’s Developer Tools.To begin our coding project, let’s activate our Python 3 programming environment',\n",
       " 'Make sure you’re in the directory where your environment is located, and run the following command:With our programming environment activated, we’ll create a new file, with nano for instance',\n",
       " 'You can name your file whatever you would like, we’ll call it nga_z_artists.py in this tutorial.Within this file, we can begin to import the libraries we’ll be using — Requests and Beautiful Soup.The Requests library allows you to make use of HTTP within your Python programs in a human readable way, and the Beautiful Soup module is designed to get web scraping done quickly.We will import both Requests and Beautiful Soup with the import statement',\n",
       " 'For Beautiful Soup, we’ll be importing it from bs4, the package in which Beautiful Soup 4 is found',\n",
       " 'With both the Requests and Beautiful Soup modules imported, we can move on to working to first collect a page and then parse it',\n",
       " 'The next step we will need to do is collect the URL of the first web page with Requests',\n",
       " 'We’ll assign the URL for the first page to the variable page by using the method requests.get().Next, we’ll create a BeautifulSoup object, or a parse tree',\n",
       " 'This object takes as its arguments the page.text document from Requests (the content of the server’s response) and then parses it from Python’s built-in html.parser.Now with our page collected, parsed, and set up as a BeautifulSoup object, we can move on to collecting the data that we would like.For this project, we’ll collect artists’ names and the relevant links available on the website',\n",
       " 'You may want to collect different data, such as the artists’ nationality and dates',\n",
       " 'Whatever data you would like to collect, you need to find out how it is described by the DOM of the web page',\n",
       " 'To do this, in your web browser, right-click — or CTRL + click on macOS — on the first artist’s name, Zabaglia, Niccola',\n",
       " 'Within the context menu that pops up, you should see a menu item similar to Inspect Element (Firefox) or Inspect (Chrome).Once you click on the relevant Inspect menu item, the tools for web developers should appear within your browser',\n",
       " 'We want to look for the class and tags associated with the artists’ names in this list',\n",
       " 'We’ll see first that the table of names is within <div> tags where class=\"BodyText\"',\n",
       " 'This is important to note so that we only search for text within this section of the web page',\n",
       " 'We also notice that the name Zabaglia, Niccola is in a link tag, since the name references a web page that describes the artist',\n",
       " 'So we will want to reference the <a> tag for links',\n",
       " 'Each artist’s name is a reference to a link',\n",
       " 'To do this, we’ll use Beautiful Soup’s find() and find_all() methods in order to pull the text of the artists’ names from the BodyText <div>',\n",
       " 'Next, at the bottom of our program file, we will want to create a for loop in order to iterate over all the artist names that we just put into the artist_name_list_items variable',\n",
       " 'We’ll print these names out with the prettify() method in order to turn the Beautiful Soup parse tree into a nicely formatted Unicode string.Let’s run the program as we have it so far:Once we do so, we’ll receive the following output:What we see in the output at this point is the full text and tags related to all of the artists’ names within the <a> tags found in the <div class=\"BodyText\"> tag on the first page, as well as some additional link text at the bottom',\n",
       " 'Since we don’t want this extra information, let’s work on removing this in the next section.So far, we have been able to collect all the link text data within one <div> section of our web page',\n",
       " 'However, we don’t want to have the bottom links that don’t reference artists’ names, so let’s work to remove that part',\n",
       " 'In order to remove the bottom links of the page, let’s again right-click and Inspect the DOM',\n",
       " 'We’ll see that the links on the bottom of the <div class=\"BodyText\"> section are contained in an HTML table: <table class=\"AlphaNav\">:We can therefore use Beautiful Soup to find the AlphaNav class and use the decompose() method to remove a tag from the parse tree and then destroy it along with its contents',\n",
       " 'We’ll use the variable last_links to reference these bottom links and add them to the program file:Now, when we run the program with the python nga_z_artist.py command, we’ll receive the following output:At this point, we see that the output no longer includes the links at the bottom of the web page, and now only displays the links associated with artists’ names',\n",
       " 'So far we have targeted the links with the artists’ names specifically, but we have the extra tag data that we don’t really want',\n",
       " 'Let’s remove that in the next section.In order to access only the actual artists’ names, we’ll want to target the contents of the <a> tags rather than print out the entire link tag.We can do this with Beautiful Soup’s .contents, which will return the tag’s children as a Python list data type.Let’s revise the for loop so that instead of printing the entire link and its tag, we’ll print the list of children (i.e',\n",
       " 'the artists’ full names):Note that we are iterating over the list above by calling on the index number of each item.We can run the program with the python command to view the following output:We have received back a list of all the artists’ names available on the first page of the letter Z',\n",
       " \"However, what if we want to also capture the URLs associated with those artists? We can extract URLs found within a page’s <a> tags by using Beautiful Soup’s get('href') method\",\n",
       " 'From the output of the links above, we know that the entire URL is not being captured, so we will concatenate the link string with the front of the URL string (in this case https://www.nga.gov).These lines we’ll also add to the for loop:When we run the program above, we’ll receive both the artists’ names and the URLs to the links that tell us more about the artists:Although we are now getting information from the website, it is currently just printing to our terminal window',\n",
       " 'Let’s instead capture this data so that we can use it elsewhere by writing it to a file.Collecting data that only lives in a terminal window is not very useful',\n",
       " 'Comma-separated values (CSV) files allows us to store tabular data in plain text, and is a common format for spreadsheets and databases',\n",
       " \"Before beginning with this section, you should familiarize yourself with how to handle plain text files in Python.First, we need to import Python’s built-in csv module along with the other modules at the top of the Python programming file:Next, we’ll create and open a file called z-artist-names.csv for us to write to (we’ll use the variable f for file here) by using the 'w' mode\",\n",
       " 'We’ll also write the top row headings: Name and Link which we’ll pass to the writerow() method as a list:Finally, within our for loop, we’ll write each row with the artists’ names and their associated links:You can see the lines for each of these tasks in the file below:When you run the program now with the python command, no output will be returned to your terminal window',\n",
       " 'Instead, a file will be created in the directory you are working in called z-artist-names.csv',\n",
       " 'Depending on what you use to open it, it may look something like this:Or, it may look more like a spreadsheet:In either case, you can now use this file to work with the data in more meaningful ways since the information you have collected is now stored in your computer’s memory.We have created a program that will pull data from the first page of the list of artists whose last names start with the letter Z',\n",
       " 'However, there are 5 pages in total of these artists available on the website.In order to collect all of these pages, we can perform more iterations with for loops',\n",
       " 'This will revise most of the code we have written so far, but will employ similar concepts.To start, we’ll want to initialize a list to hold the pages:We will populate this initialized list with the following for loop:Earlier in this tutorial, we noted that we should pay attention to the total number of pages there are that contain artists’ names starting with the letter Z (or whatever letter we’re using)',\n",
       " 'Since there are 5 pages for the letter Z, we constructed the for loop above with a range of 1 to 6 so that it will iterate through each of the 5 pages',\n",
       " 'For this specific web site, the URLs begin with the string https://www.nga.gov/collection/anZ and then are followed with a number for the page (which will be the integer i from the for loop that we convert to a string) and end with .htm',\n",
       " 'We will concatenate these strings together and then append the result to the pages list.In addition to this loop, we’ll have a second loop that will go through each of the pages above',\n",
       " 'The code in this for loop will look similar to the code we have created so far, as it is doing the task we completed for the first page of the letter Z artists for each of the 5 pages total',\n",
       " 'Note that because we have put the original program into the second for loop, we now have the original loop as a nested for loop contained in it',\n",
       " 'The two for loops will look like this:In the code above, you should see that the first for loop is iterating over the pages and the second for loop is scraping data from each of those pages and then is adding the artists’ names and links line by line through each row of each page',\n",
       " 'These two for loops come below the import statements, the CSV file creation and writer (with the line for writing the headers of the file), and the initialization of the pages variable (assigned to a list).Within the greater context of the programming file, the complete code looks like this:Since this program is doing a bit of work, it will take a little while to create the CSV file',\n",
       " 'Once it is done, the output will be complete, showing the artists’ names and their associated links from Zabaglia, Niccola to Zykmund, Václav.When scraping web pages, it is important to remain considerate of the servers you are grabbing information from',\n",
       " 'Check to see if a site has terms of service or terms of use that pertains to web scraping',\n",
       " 'Also, check to see if a site has an API that allows you to grab data before scraping it yourself',\n",
       " 'Be sure to not continuously hit servers to gather data',\n",
       " 'Once you have collected what you need from a site, run scripts that will go over the data locally rather than burden someone else’s servers',\n",
       " 'Additionally, it is a good idea to scrape with a header that has your name and email so that a website can identify you and follow up if they have any questions',\n",
       " 'An example of a header you can use with the Python Requests library is as follows:Using headers with identifiable information ensures that the people who go over a server’s logs can reach out to you.This tutorial went through using Python and Beautiful Soup to scrape data from a website',\n",
       " 'We stored the text that we gathered within a CSV file.You can continue working on this project by collecting more data and making your CSV file more robust',\n",
       " 'For example, you may want to include the nationalities and years of each artist',\n",
       " 'You can also use what you have learned to scrape data from other websites',\n",
       " 'To continue learning about pulling information from the web, read our tutorial “How To Crawl A Web Page with Scrapy and Python 3.”Simple setup',\n",
       " 'Full root access',\n",
       " 'Straightforward pricing.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Get', 'the', 'latest', 'tutorials', 'on', 'SysAdmin', 'and', 'open', 'source', 'topics.', 'Many', 'data', 'analysis,', 'big', 'data,', 'and', 'machine', 'learning', 'projects', 'require', 'scraping', 'websites', 'to', 'gather', 'the', 'data', 'that', 'you’ll', 'be', 'working', 'with.', 'The', 'Python', 'programming', 'language', 'is', 'widely', 'used', 'in', 'the', 'data', 'science', 'community,', 'and', 'therefore', 'has', 'an', 'ecosystem', 'of', 'modules', 'and', 'tools', 'that', 'you', 'can', 'use', 'in', 'your', 'own', 'projects.', 'In', 'this', 'tutorial', 'we', 'will', 'be', 'focusing', 'on', 'the', 'Beautiful', 'Soup', 'module.', 'Beautiful', 'Soup,', 'an', 'allusion', 'to', 'the', 'Mock', 'Turtle’s', 'song', 'found', 'in', 'Chapter', '10', 'of', 'Lewis', 'Carroll’s', 'Alice’s', 'Adventures', 'in', 'Wonderland,', 'is', 'a', 'Python', 'library', 'that', 'allows', 'for', 'quick', 'turnaround', 'on', 'web', 'scraping', 'projects.', 'Currently', 'available', 'as', 'Beautiful', 'Soup', '4', 'and', 'compatible', 'with', 'both', 'Python', '2.7', 'and', 'Python', '3,', 'Beautiful', 'Soup', 'creates', 'a', 'parse', 'tree', 'from', 'parsed', 'HTML', 'and', 'XML', 'documents', '(including', 'documents', 'with', 'non-closed', 'tags', 'or', 'tag', 'soup', 'and', 'other', 'malformed', 'markup).', 'In', 'this', 'tutorial,', 'we', 'will', 'collect', 'and', 'parse', 'a', 'web', 'page', 'in', 'order', 'to', 'grab', 'textual', 'data', 'and', 'write', 'the', 'information', 'we', 'have', 'gathered', 'to', 'a', 'CSV', 'file.', 'Before', 'working', 'on', 'this', 'tutorial,', 'you', 'should', 'have', 'a', 'local', 'or', 'server-based', 'Python', 'programming', 'environment', 'set', 'up', 'on', 'your', 'machine.You', 'should', 'have', 'the', 'Requests', 'and', 'Beautiful', 'Soup', 'modules', 'installed,', 'which', 'you', 'can', 'achieve', 'by', 'following', 'our', 'tutorial', '“How', 'To', 'Work', 'with', 'Web', 'Data', 'Using', 'Requests', 'and', 'Beautiful', 'Soup', 'with', 'Python', '3.”', 'It', 'would', 'also', 'be', 'useful', 'to', 'have', 'a', 'working', 'familiarity', 'with', 'these', 'modules.', 'Additionally,', 'since', 'we', 'will', 'be', 'working', 'with', 'data', 'scraped', 'from', 'the', 'web,', 'you', 'should', 'be', 'comfortable', 'with', 'HTML', 'structure', 'and', 'tagging.', 'In', 'this', 'tutorial,', 'we’ll', 'be', 'working', 'with', 'data', 'from', 'the', 'official', 'website', 'of', 'the', 'National', 'Gallery', 'of', 'Art', 'in', 'the', 'United', 'States.', 'The', 'National', 'Gallery', 'is', 'an', 'art', 'museum', 'located', 'on', 'the', 'National', 'Mall', 'in', 'Washington,', 'D.C.', 'It', 'holds', 'over', '120,000', 'pieces', 'dated', 'from', 'the', 'Renaissance', 'to', 'the', 'present', 'day', 'done', 'by', 'more', 'than', '13,000', 'artists.We', 'would', 'like', 'to', 'search', 'the', 'Index', 'of', 'Artists,', 'which', 'is', 'available', 'at', 'https://www.nga.gov/collection/an.shtm.', 'Since', 'we’ll', 'be', 'doing', 'this', 'project', 'in', 'order', 'to', 'learn', 'about', 'web', 'scraping', 'with', 'Beautiful', 'Soup,', 'we', 'don’t', 'need', 'to', 'pull', 'too', 'much', 'data', 'from', 'the', 'site,', 'so', 'let’s', 'limit', 'the', 'scope', 'of', 'the', 'artist', 'data', 'we', 'are', 'looking', 'to', 'scrape.', 'Let’s', 'therefore', 'choose', 'one', 'letter', '—', 'in', 'our', 'example', 'we’ll', 'choose', 'the', 'letter', 'Z', '—', 'and', 'we’ll', 'see', 'a', 'page', 'that', 'looks', 'like', 'this:In', 'the', 'page', 'above,', 'we', 'see', 'that', 'the', 'first', 'artist', 'listed', 'at', 'the', 'time', 'of', 'writing', 'is', 'Zabaglia,', 'Niccola,', 'which', 'is', 'a', 'good', 'thing', 'to', 'note', 'for', 'when', 'we', 'start', 'pulling', 'data.', 'We’ll', 'start', 'by', 'working', 'with', 'this', 'first', 'page,', 'with', 'the', 'following', 'URL', 'for', 'the', 'letter', 'Z:It', 'is', 'important', 'to', 'note', 'for', 'later', 'how', 'many', 'pages', 'total', 'there', 'are', 'for', 'the', 'letter', 'you', 'are', 'choosing', 'to', 'list,', 'which', 'you', 'can', 'discover', 'by', 'clicking', 'through', 'to', 'the', 'last', 'page', 'of', 'artists.', 'In', 'this', 'case,', 'there', 'are', '5', 'pages', 'total,', 'and', 'the', 'last', 'artist', 'listed', 'at', 'the', 'time', 'of', 'writing', 'is', 'Zykmund,', 'Václav.', 'The', 'last', 'page', 'of', 'Z', 'artists', 'has', 'the', 'following', 'URL:To', 'begin', 'to', 'familiarize', 'yourself', 'with', 'the', 'DOM', 'of', 'this', 'web', 'page,', 'you', 'can', 'open', 'your', 'browser’s', 'Developer', 'Tools.To', 'begin', 'our', 'coding', 'project,', 'let’s', 'activate', 'our', 'Python', '3', 'programming', 'environment.', 'Make', 'sure', 'you’re', 'in', 'the', 'directory', 'where', 'your', 'environment', 'is', 'located,', 'and', 'run', 'the', 'following', 'command:With', 'our', 'programming', 'environment', 'activated,', 'we’ll', 'create', 'a', 'new', 'file,', 'with', 'nano', 'for', 'instance.', 'You', 'can', 'name', 'your', 'file', 'whatever', 'you', 'would', 'like,', 'we’ll', 'call', 'it', 'nga_z_artists.py', 'in', 'this', 'tutorial.Within', 'this', 'file,', 'we', 'can', 'begin', 'to', 'import', 'the', 'libraries', 'we’ll', 'be', 'using', '—', 'Requests', 'and', 'Beautiful', 'Soup.The', 'Requests', 'library', 'allows', 'you', 'to', 'make', 'use', 'of', 'HTTP', 'within', 'your', 'Python', 'programs', 'in', 'a', 'human', 'readable', 'way,', 'and', 'the', 'Beautiful', 'Soup', 'module', 'is', 'designed', 'to', 'get', 'web', 'scraping', 'done', 'quickly.We', 'will', 'import', 'both', 'Requests', 'and', 'Beautiful', 'Soup', 'with', 'the', 'import', 'statement.', 'For', 'Beautiful', 'Soup,', 'we’ll', 'be', 'importing', 'it', 'from', 'bs4,', 'the', 'package', 'in', 'which', 'Beautiful', 'Soup', '4', 'is', 'found.', 'With', 'both', 'the', 'Requests', 'and', 'Beautiful', 'Soup', 'modules', 'imported,', 'we', 'can', 'move', 'on', 'to', 'working', 'to', 'first', 'collect', 'a', 'page', 'and', 'then', 'parse', 'it.', 'The', 'next', 'step', 'we', 'will', 'need', 'to', 'do', 'is', 'collect', 'the', 'URL', 'of', 'the', 'first', 'web', 'page', 'with', 'Requests.', 'We’ll', 'assign', 'the', 'URL', 'for', 'the', 'first', 'page', 'to', 'the', 'variable', 'page', 'by', 'using', 'the', 'method', 'requests.get().Next,', 'we’ll', 'create', 'a', 'BeautifulSoup', 'object,', 'or', 'a', 'parse', 'tree.', 'This', 'object', 'takes', 'as', 'its', 'arguments', 'the', 'page.text', 'document', 'from', 'Requests', '(the', 'content', 'of', 'the', 'server’s', 'response)', 'and', 'then', 'parses', 'it', 'from', 'Python’s', 'built-in', 'html.parser.Now', 'with', 'our', 'page', 'collected,', 'parsed,', 'and', 'set', 'up', 'as', 'a', 'BeautifulSoup', 'object,', 'we', 'can', 'move', 'on', 'to', 'collecting', 'the', 'data', 'that', 'we', 'would', 'like.For', 'this', 'project,', 'we’ll', 'collect', 'artists’', 'names', 'and', 'the', 'relevant', 'links', 'available', 'on', 'the', 'website.', 'You', 'may', 'want', 'to', 'collect', 'different', 'data,', 'such', 'as', 'the', 'artists’', 'nationality', 'and', 'dates.', 'Whatever', 'data', 'you', 'would', 'like', 'to', 'collect,', 'you', 'need', 'to', 'find', 'out', 'how', 'it', 'is', 'described', 'by', 'the', 'DOM', 'of', 'the', 'web', 'page.', 'To', 'do', 'this,', 'in', 'your', 'web', 'browser,', 'right-click', '—', 'or', 'CTRL', '+', 'click', 'on', 'macOS', '—', 'on', 'the', 'first', 'artist’s', 'name,', 'Zabaglia,', 'Niccola.', 'Within', 'the', 'context', 'menu', 'that', 'pops', 'up,', 'you', 'should', 'see', 'a', 'menu', 'item', 'similar', 'to', 'Inspect', 'Element', '(Firefox)', 'or', 'Inspect', '(Chrome).Once', 'you', 'click', 'on', 'the', 'relevant', 'Inspect', 'menu', 'item,', 'the', 'tools', 'for', 'web', 'developers', 'should', 'appear', 'within', 'your', 'browser.', 'We', 'want', 'to', 'look', 'for', 'the', 'class', 'and', 'tags', 'associated', 'with', 'the', 'artists’', 'names', 'in', 'this', 'list.', 'We’ll', 'see', 'first', 'that', 'the', 'table', 'of', 'names', 'is', 'within', '<div>', 'tags', 'where', 'class=\"BodyText\".', 'This', 'is', 'important', 'to', 'note', 'so', 'that', 'we', 'only', 'search', 'for', 'text', 'within', 'this', 'section', 'of', 'the', 'web', 'page.', 'We', 'also', 'notice', 'that', 'the', 'name', 'Zabaglia,', 'Niccola', 'is', 'in', 'a', 'link', 'tag,', 'since', 'the', 'name', 'references', 'a', 'web', 'page', 'that', 'describes', 'the', 'artist.', 'So', 'we', 'will', 'want', 'to', 'reference', 'the', '<a>', 'tag', 'for', 'links.', 'Each', 'artist’s', 'name', 'is', 'a', 'reference', 'to', 'a', 'link.', 'To', 'do', 'this,', 'we’ll', 'use', 'Beautiful', 'Soup’s', 'find()', 'and', 'find_all()', 'methods', 'in', 'order', 'to', 'pull', 'the', 'text', 'of', 'the', 'artists’', 'names', 'from', 'the', 'BodyText', '<div>.', 'Next,', 'at', 'the', 'bottom', 'of', 'our', 'program', 'file,', 'we', 'will', 'want', 'to', 'create', 'a', 'for', 'loop', 'in', 'order', 'to', 'iterate', 'over', 'all', 'the', 'artist', 'names', 'that', 'we', 'just', 'put', 'into', 'the', 'artist_name_list_items', 'variable.', 'We’ll', 'print', 'these', 'names', 'out', 'with', 'the', 'prettify()', 'method', 'in', 'order', 'to', 'turn', 'the', 'Beautiful', 'Soup', 'parse', 'tree', 'into', 'a', 'nicely', 'formatted', 'Unicode', 'string.Let’s', 'run', 'the', 'program', 'as', 'we', 'have', 'it', 'so', 'far:Once', 'we', 'do', 'so,', 'we’ll', 'receive', 'the', 'following', 'output:What', 'we', 'see', 'in', 'the', 'output', 'at', 'this', 'point', 'is', 'the', 'full', 'text', 'and', 'tags', 'related', 'to', 'all', 'of', 'the', 'artists’', 'names', 'within', 'the', '<a>', 'tags', 'found', 'in', 'the', '<div', 'class=\"BodyText\">', 'tag', 'on', 'the', 'first', 'page,', 'as', 'well', 'as', 'some', 'additional', 'link', 'text', 'at', 'the', 'bottom.', 'Since', 'we', 'don’t', 'want', 'this', 'extra', 'information,', 'let’s', 'work', 'on', 'removing', 'this', 'in', 'the', 'next', 'section.So', 'far,', 'we', 'have', 'been', 'able', 'to', 'collect', 'all', 'the', 'link', 'text', 'data', 'within', 'one', '<div>', 'section', 'of', 'our', 'web', 'page.', 'However,', 'we', 'don’t', 'want', 'to', 'have', 'the', 'bottom', 'links', 'that', 'don’t', 'reference', 'artists’', 'names,', 'so', 'let’s', 'work', 'to', 'remove', 'that', 'part.', 'In', 'order', 'to', 'remove', 'the', 'bottom', 'links', 'of', 'the', 'page,', 'let’s', 'again', 'right-click', 'and', 'Inspect', 'the', 'DOM.', 'We’ll', 'see', 'that', 'the', 'links', 'on', 'the', 'bottom', 'of', 'the', '<div', 'class=\"BodyText\">', 'section', 'are', 'contained', 'in', 'an', 'HTML', 'table:', '<table', 'class=\"AlphaNav\">:We', 'can', 'therefore', 'use', 'Beautiful', 'Soup', 'to', 'find', 'the', 'AlphaNav', 'class', 'and', 'use', 'the', 'decompose()', 'method', 'to', 'remove', 'a', 'tag', 'from', 'the', 'parse', 'tree', 'and', 'then', 'destroy', 'it', 'along', 'with', 'its', 'contents.', 'We’ll', 'use', 'the', 'variable', 'last_links', 'to', 'reference', 'these', 'bottom', 'links', 'and', 'add', 'them', 'to', 'the', 'program', 'file:Now,', 'when', 'we', 'run', 'the', 'program', 'with', 'the', 'python', 'nga_z_artist.py', 'command,', 'we’ll', 'receive', 'the', 'following', 'output:At', 'this', 'point,', 'we', 'see', 'that', 'the', 'output', 'no', 'longer', 'includes', 'the', 'links', 'at', 'the', 'bottom', 'of', 'the', 'web', 'page,', 'and', 'now', 'only', 'displays', 'the', 'links', 'associated', 'with', 'artists’', 'names.', 'So', 'far', 'we', 'have', 'targeted', 'the', 'links', 'with', 'the', 'artists’', 'names', 'specifically,', 'but', 'we', 'have', 'the', 'extra', 'tag', 'data', 'that', 'we', 'don’t', 'really', 'want.', 'Let’s', 'remove', 'that', 'in', 'the', 'next', 'section.In', 'order', 'to', 'access', 'only', 'the', 'actual', 'artists’', 'names,', 'we’ll', 'want', 'to', 'target', 'the', 'contents', 'of', 'the', '<a>', 'tags', 'rather', 'than', 'print', 'out', 'the', 'entire', 'link', 'tag.We', 'can', 'do', 'this', 'with', 'Beautiful', 'Soup’s', '.contents,', 'which', 'will', 'return', 'the', 'tag’s', 'children', 'as', 'a', 'Python', 'list', 'data', 'type.Let’s', 'revise', 'the', 'for', 'loop', 'so', 'that', 'instead', 'of', 'printing', 'the', 'entire', 'link', 'and', 'its', 'tag,', 'we’ll', 'print', 'the', 'list', 'of', 'children', '(i.e.', 'the', 'artists’', 'full', 'names):Note', 'that', 'we', 'are', 'iterating', 'over', 'the', 'list', 'above', 'by', 'calling', 'on', 'the', 'index', 'number', 'of', 'each', 'item.We', 'can', 'run', 'the', 'program', 'with', 'the', 'python', 'command', 'to', 'view', 'the', 'following', 'output:We', 'have', 'received', 'back', 'a', 'list', 'of', 'all', 'the', 'artists’', 'names', 'available', 'on', 'the', 'first', 'page', 'of', 'the', 'letter', 'Z.', 'However,', 'what', 'if', 'we', 'want', 'to', 'also', 'capture', 'the', 'URLs', 'associated', 'with', 'those', 'artists?', 'We', 'can', 'extract', 'URLs', 'found', 'within', 'a', 'page’s', '<a>', 'tags', 'by', 'using', 'Beautiful', 'Soup’s', \"get('href')\", 'method.', 'From', 'the', 'output', 'of', 'the', 'links', 'above,', 'we', 'know', 'that', 'the', 'entire', 'URL', 'is', 'not', 'being', 'captured,', 'so', 'we', 'will', 'concatenate', 'the', 'link', 'string', 'with', 'the', 'front', 'of', 'the', 'URL', 'string', '(in', 'this', 'case', 'https://www.nga.gov).These', 'lines', 'we’ll', 'also', 'add', 'to', 'the', 'for', 'loop:When', 'we', 'run', 'the', 'program', 'above,', 'we’ll', 'receive', 'both', 'the', 'artists’', 'names', 'and', 'the', 'URLs', 'to', 'the', 'links', 'that', 'tell', 'us', 'more', 'about', 'the', 'artists:Although', 'we', 'are', 'now', 'getting', 'information', 'from', 'the', 'website,', 'it', 'is', 'currently', 'just', 'printing', 'to', 'our', 'terminal', 'window.', 'Let’s', 'instead', 'capture', 'this', 'data', 'so', 'that', 'we', 'can', 'use', 'it', 'elsewhere', 'by', 'writing', 'it', 'to', 'a', 'file.Collecting', 'data', 'that', 'only', 'lives', 'in', 'a', 'terminal', 'window', 'is', 'not', 'very', 'useful.', 'Comma-separated', 'values', '(CSV)', 'files', 'allows', 'us', 'to', 'store', 'tabular', 'data', 'in', 'plain', 'text,', 'and', 'is', 'a', 'common', 'format', 'for', 'spreadsheets', 'and', 'databases.', 'Before', 'beginning', 'with', 'this', 'section,', 'you', 'should', 'familiarize', 'yourself', 'with', 'how', 'to', 'handle', 'plain', 'text', 'files', 'in', 'Python.First,', 'we', 'need', 'to', 'import', 'Python’s', 'built-in', 'csv', 'module', 'along', 'with', 'the', 'other', 'modules', 'at', 'the', 'top', 'of', 'the', 'Python', 'programming', 'file:Next,', 'we’ll', 'create', 'and', 'open', 'a', 'file', 'called', 'z-artist-names.csv', 'for', 'us', 'to', 'write', 'to', '(we’ll', 'use', 'the', 'variable', 'f', 'for', 'file', 'here)', 'by', 'using', 'the', \"'w'\", 'mode.', 'We’ll', 'also', 'write', 'the', 'top', 'row', 'headings:', 'Name', 'and', 'Link', 'which', 'we’ll', 'pass', 'to', 'the', 'writerow()', 'method', 'as', 'a', 'list:Finally,', 'within', 'our', 'for', 'loop,', 'we’ll', 'write', 'each', 'row', 'with', 'the', 'artists’', 'names', 'and', 'their', 'associated', 'links:You', 'can', 'see', 'the', 'lines', 'for', 'each', 'of', 'these', 'tasks', 'in', 'the', 'file', 'below:When', 'you', 'run', 'the', 'program', 'now', 'with', 'the', 'python', 'command,', 'no', 'output', 'will', 'be', 'returned', 'to', 'your', 'terminal', 'window.', 'Instead,', 'a', 'file', 'will', 'be', 'created', 'in', 'the', 'directory', 'you', 'are', 'working', 'in', 'called', 'z-artist-names.csv.', 'Depending', 'on', 'what', 'you', 'use', 'to', 'open', 'it,', 'it', 'may', 'look', 'something', 'like', 'this:Or,', 'it', 'may', 'look', 'more', 'like', 'a', 'spreadsheet:In', 'either', 'case,', 'you', 'can', 'now', 'use', 'this', 'file', 'to', 'work', 'with', 'the', 'data', 'in', 'more', 'meaningful', 'ways', 'since', 'the', 'information', 'you', 'have', 'collected', 'is', 'now', 'stored', 'in', 'your', 'computer’s', 'memory.We', 'have', 'created', 'a', 'program', 'that', 'will', 'pull', 'data', 'from', 'the', 'first', 'page', 'of', 'the', 'list', 'of', 'artists', 'whose', 'last', 'names', 'start', 'with', 'the', 'letter', 'Z.', 'However,', 'there', 'are', '5', 'pages', 'in', 'total', 'of', 'these', 'artists', 'available', 'on', 'the', 'website.In', 'order', 'to', 'collect', 'all', 'of', 'these', 'pages,', 'we', 'can', 'perform', 'more', 'iterations', 'with', 'for', 'loops.', 'This', 'will', 'revise', 'most', 'of', 'the', 'code', 'we', 'have', 'written', 'so', 'far,', 'but', 'will', 'employ', 'similar', 'concepts.To', 'start,', 'we’ll', 'want', 'to', 'initialize', 'a', 'list', 'to', 'hold', 'the', 'pages:We', 'will', 'populate', 'this', 'initialized', 'list', 'with', 'the', 'following', 'for', 'loop:Earlier', 'in', 'this', 'tutorial,', 'we', 'noted', 'that', 'we', 'should', 'pay', 'attention', 'to', 'the', 'total', 'number', 'of', 'pages', 'there', 'are', 'that', 'contain', 'artists’', 'names', 'starting', 'with', 'the', 'letter', 'Z', '(or', 'whatever', 'letter', 'we’re', 'using).', 'Since', 'there', 'are', '5', 'pages', 'for', 'the', 'letter', 'Z,', 'we', 'constructed', 'the', 'for', 'loop', 'above', 'with', 'a', 'range', 'of', '1', 'to', '6', 'so', 'that', 'it', 'will', 'iterate', 'through', 'each', 'of', 'the', '5', 'pages.', 'For', 'this', 'specific', 'web', 'site,', 'the', 'URLs', 'begin', 'with', 'the', 'string', 'https://www.nga.gov/collection/anZ', 'and', 'then', 'are', 'followed', 'with', 'a', 'number', 'for', 'the', 'page', '(which', 'will', 'be', 'the', 'integer', 'i', 'from', 'the', 'for', 'loop', 'that', 'we', 'convert', 'to', 'a', 'string)', 'and', 'end', 'with', '.htm.', 'We', 'will', 'concatenate', 'these', 'strings', 'together', 'and', 'then', 'append', 'the', 'result', 'to', 'the', 'pages', 'list.In', 'addition', 'to', 'this', 'loop,', 'we’ll', 'have', 'a', 'second', 'loop', 'that', 'will', 'go', 'through', 'each', 'of', 'the', 'pages', 'above.', 'The', 'code', 'in', 'this', 'for', 'loop', 'will', 'look', 'similar', 'to', 'the', 'code', 'we', 'have', 'created', 'so', 'far,', 'as', 'it', 'is', 'doing', 'the', 'task', 'we', 'completed', 'for', 'the', 'first', 'page', 'of', 'the', 'letter', 'Z', 'artists', 'for', 'each', 'of', 'the', '5', 'pages', 'total.', 'Note', 'that', 'because', 'we', 'have', 'put', 'the', 'original', 'program', 'into', 'the', 'second', 'for', 'loop,', 'we', 'now', 'have', 'the', 'original', 'loop', 'as', 'a', 'nested', 'for', 'loop', 'contained', 'in', 'it.', 'The', 'two', 'for', 'loops', 'will', 'look', 'like', 'this:In', 'the', 'code', 'above,', 'you', 'should', 'see', 'that', 'the', 'first', 'for', 'loop', 'is', 'iterating', 'over', 'the', 'pages', 'and', 'the', 'second', 'for', 'loop', 'is', 'scraping', 'data', 'from', 'each', 'of', 'those', 'pages', 'and', 'then', 'is', 'adding', 'the', 'artists’', 'names', 'and', 'links', 'line', 'by', 'line', 'through', 'each', 'row', 'of', 'each', 'page.', 'These', 'two', 'for', 'loops', 'come', 'below', 'the', 'import', 'statements,', 'the', 'CSV', 'file', 'creation', 'and', 'writer', '(with', 'the', 'line', 'for', 'writing', 'the', 'headers', 'of', 'the', 'file),', 'and', 'the', 'initialization', 'of', 'the', 'pages', 'variable', '(assigned', 'to', 'a', 'list).Within', 'the', 'greater', 'context', 'of', 'the', 'programming', 'file,', 'the', 'complete', 'code', 'looks', 'like', 'this:Since', 'this', 'program', 'is', 'doing', 'a', 'bit', 'of', 'work,', 'it', 'will', 'take', 'a', 'little', 'while', 'to', 'create', 'the', 'CSV', 'file.', 'Once', 'it', 'is', 'done,', 'the', 'output', 'will', 'be', 'complete,', 'showing', 'the', 'artists’', 'names', 'and', 'their', 'associated', 'links', 'from', 'Zabaglia,', 'Niccola', 'to', 'Zykmund,', 'Václav.When', 'scraping', 'web', 'pages,', 'it', 'is', 'important', 'to', 'remain', 'considerate', 'of', 'the', 'servers', 'you', 'are', 'grabbing', 'information', 'from.', 'Check', 'to', 'see', 'if', 'a', 'site', 'has', 'terms', 'of', 'service', 'or', 'terms', 'of', 'use', 'that', 'pertains', 'to', 'web', 'scraping.', 'Also,', 'check', 'to', 'see', 'if', 'a', 'site', 'has', 'an', 'API', 'that', 'allows', 'you', 'to', 'grab', 'data', 'before', 'scraping', 'it', 'yourself.', 'Be', 'sure', 'to', 'not', 'continuously', 'hit', 'servers', 'to', 'gather', 'data.', 'Once', 'you', 'have', 'collected', 'what', 'you', 'need', 'from', 'a', 'site,', 'run', 'scripts', 'that', 'will', 'go', 'over', 'the', 'data', 'locally', 'rather', 'than', 'burden', 'someone', 'else’s', 'servers.', 'Additionally,', 'it', 'is', 'a', 'good', 'idea', 'to', 'scrape', 'with', 'a', 'header', 'that', 'has', 'your', 'name', 'and', 'email', 'so', 'that', 'a', 'website', 'can', 'identify', 'you', 'and', 'follow', 'up', 'if', 'they', 'have', 'any', 'questions.', 'An', 'example', 'of', 'a', 'header', 'you', 'can', 'use', 'with', 'the', 'Python', 'Requests', 'library', 'is', 'as', 'follows:Using', 'headers', 'with', 'identifiable', 'information', 'ensures', 'that', 'the', 'people', 'who', 'go', 'over', 'a', 'server’s', 'logs', 'can', 'reach', 'out', 'to', 'you.This', 'tutorial', 'went', 'through', 'using', 'Python', 'and', 'Beautiful', 'Soup', 'to', 'scrape', 'data', 'from', 'a', 'website.', 'We', 'stored', 'the', 'text', 'that', 'we', 'gathered', 'within', 'a', 'CSV', 'file.You', 'can', 'continue', 'working', 'on', 'this', 'project', 'by', 'collecting', 'more', 'data', 'and', 'making', 'your', 'CSV', 'file', 'more', 'robust.', 'For', 'example,', 'you', 'may', 'want', 'to', 'include', 'the', 'nationalities', 'and', 'years', 'of', 'each', 'artist.', 'You', 'can', 'also', 'use', 'what', 'you', 'have', 'learned', 'to', 'scrape', 'data', 'from', 'other', 'websites.', 'To', 'continue', 'learning', 'about', 'pulling', 'information', 'from', 'the', 'web,', 'read', 'our', 'tutorial', '“How', 'To', 'Crawl', 'A', 'Web', 'Page', 'with', 'Scrapy', 'and', 'Python', '3.”Simple', 'setup.', 'Full', 'root', 'access.', 'Straightforward', 'pricing.']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e=e.split()\n",
    "print(e)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
